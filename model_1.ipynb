{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e07478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "layers = keras.layers\n",
    "Model = keras.Model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ea17018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_spark_session():\n",
    "    \"\"\"Configurar Spark para procesar 169M+ registros\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"UTD19_BigData_TrafficPrediction\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"12g\") \\\n",
    "        .config(\"spark.executor.cores\", \"2\") \\\n",
    "        .config(\"spark.executor.instances\", \"2\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    #Ajuste para evitar errores en Windows con Hadoop NativeIO\n",
    "    spark.conf.set(\"spark.hadoop.io.nativeio.enable\", \"false\")\n",
    "    \n",
    "    print(f\"‚úÖ Spark configurado - Versi√≥n: {spark.version}\")\n",
    "    print(f\"üìä Configuraci√≥n: 8 executors, 16GB memoria cada uno\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d7f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UTD19_ETL_Pipeline:\n",
    "    \"\"\"Pipeline ETL completo para dataset UTD19 (134M+ registros)\"\"\"\n",
    "    \n",
    "    def __init__(self, spark_session):\n",
    "        self.spark = spark_session\n",
    "        self.quality_metrics = {}\n",
    "        \n",
    "    def run_complete_etl(self, input_path=\"./data/raw/\", output_path=\"./data/processed/\"):\n",
    "        \"\"\"Ejecutar ETL completo: Extract -> Transform -> Load\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"üöÄ INICIANDO ETL PIPELINE COMPLETO UTD19\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # EXTRACT\n",
    "        raw_datasets = self.extract_raw_data(input_path)\n",
    "        \n",
    "        # TRANSFORM\n",
    "        clean_datasets = self.transform_data(raw_datasets)\n",
    "        \n",
    "        # LOAD\n",
    "        self.load_clean_data(clean_datasets, output_path)\n",
    "        \n",
    "        # QUALITY REPORT\n",
    "        self.generate_quality_report()\n",
    "        \n",
    "        return clean_datasets\n",
    "    \n",
    "    def extract_raw_data(self, input_path):\n",
    "        \"\"\"Extracci√≥n de datos con validaci√≥n inicial\"\"\"\n",
    "        print(\"\\nüì• FASE 1: EXTRACCI√ìN DE DATOS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Cargar datasets principales\n",
    "        print(\"üîÑ Cargando detectores...\")\n",
    "        detectors_df = self.spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"false\") \\\n",
    "            .csv(f\"{input_path}detectors_public.csv\")\n",
    "        \n",
    "        print(\"üîÑ Cargando links...\")\n",
    "        links_df = self.spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"false\") \\\n",
    "            .csv(f\"{input_path}links.csv\")\n",
    "        \n",
    "        print(\"üîÑ Cargando UTD19 principal (puede tomar varios minutos)...\")\n",
    "        utd19_df = self.spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"false\") \\\n",
    "            .csv(f\"{input_path}utd19_u.csv\") \\\n",
    "            .repartition(400, \"city\", \"detid\")  # Optimizar particionado\n",
    "        \n",
    "        # Estad√≠sticas iniciales\n",
    "        detectors_count = detectors_df.count()\n",
    "        links_count = links_df.count()\n",
    "        utd19_count = utd19_df.count()\n",
    "        \n",
    "        print(f\"‚úÖ Detectores: {detectors_count:,} registros\")\n",
    "        print(f\"‚úÖ Links: {links_count:,} registros\") \n",
    "        print(f\"‚úÖ UTD19: {utd19_count:,} registros\")\n",
    "        \n",
    "        self.quality_metrics['raw_counts'] = {\n",
    "            'detectors': detectors_count,\n",
    "            'links': links_count,\n",
    "            'utd19': utd19_count\n",
    "        }\n",
    "        \n",
    "        return {'detectors': detectors_df, 'links': links_df, 'utd19': utd19_df}\n",
    "    \n",
    "    def transform_data(self, raw_datasets):\n",
    "        \"\"\"Transformaciones de limpieza y enriquecimiento\"\"\"\n",
    "        print(\"\\nüîß FASE 2: TRANSFORMACI√ìN DE DATOS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # 1. Limpieza de tipos de datos\n",
    "        print(\"üìã Paso 1: Enforcement de tipos de datos...\")\n",
    "        clean_datasets = self._enforce_data_types(raw_datasets)\n",
    "        \n",
    "        # 2. Validaci√≥n de reglas de negocio\n",
    "        print(\"‚úÖ Paso 2: Aplicaci√≥n de reglas de negocio...\")\n",
    "        validated_datasets = self._apply_business_rules(clean_datasets)\n",
    "        \n",
    "        # 3. Tratamiento de valores faltantes\n",
    "        print(\"üîß Paso 3: Tratamiento de valores faltantes...\")\n",
    "        imputed_datasets = self._handle_missing_values(validated_datasets)\n",
    "        \n",
    "        # 4. Detecci√≥n y tratamiento de outliers\n",
    "        print(\"üìä Paso 4: Detecci√≥n de outliers...\")\n",
    "        final_datasets = self._detect_and_handle_outliers(imputed_datasets)\n",
    "        \n",
    "        # 5. Enriquecimiento con features derivados\n",
    "        print(\"üéØ Paso 5: Creaci√≥n de features derivados...\")\n",
    "        enriched_datasets = self._create_derived_features(final_datasets)\n",
    "        \n",
    "        return enriched_datasets\n",
    "    \n",
    "    def _enforce_data_types(self, datasets):\n",
    "        \"\"\"Conversi√≥n robusta de tipos de datos\"\"\"\n",
    "        print(\"   üîÑ Convirtiendo tipos de datos...\")\n",
    "        \n",
    "        # Esquema para UTD19 principal\n",
    "        utd19_schema = {\n",
    "            'detid': 'double',\n",
    "            'city': 'string', \n",
    "            'day': 'string',\n",
    "            'interval': 'double',\n",
    "            'flow': 'double',\n",
    "            'speed': 'double',\n",
    "            'occ': 'double',\n",
    "            'error': 'string'\n",
    "        }\n",
    "        \n",
    "        # Esquema para detectores\n",
    "        detectors_schema = {\n",
    "            'detid': 'double',\n",
    "            'citycode': 'string',\n",
    "            'lat': 'double',\n",
    "            'long': 'double', \n",
    "            'lanes': 'double',\n",
    "            'pos': 'double',\n",
    "            'limit': 'double',\n",
    "            'length': 'double',\n",
    "            'linkid': 'double',\n",
    "            'fclass': 'string',\n",
    "            'road': 'string'\n",
    "        }\n",
    "        \n",
    "        # Aplicar schemas con try_cast para robustez\n",
    "        clean_utd19 = self._apply_schema_robust(datasets['utd19'], utd19_schema)\n",
    "        clean_detectors = self._apply_schema_robust(datasets['detectors'], detectors_schema)\n",
    "        \n",
    "        # Links mantener como est√° por simplicidad\n",
    "        clean_links = datasets['links']\n",
    "        \n",
    "        return {\n",
    "            'utd19': clean_utd19,\n",
    "            'detectors': clean_detectors, \n",
    "            'links': clean_links\n",
    "        }\n",
    "    \n",
    "    def _apply_schema_robust(self, df, schema):\n",
    "        \"\"\"Aplicar schema con manejo robusto de errores - CORREGIDO\"\"\"\n",
    "        for column, data_type in schema.items():\n",
    "            if column in df.columns:\n",
    "                if data_type == 'int':\n",
    "                    # Para enteros: convertir a double primero, luego a int\n",
    "                    df = df.withColumn(column, \n",
    "                        when(col(column).isNull(), lit(None))\n",
    "                        .when(col(column).rlike(\"^-?[0-9]*\\\\.?[0-9]+$\"), \n",
    "                            col(column).cast(\"double\").cast(\"int\"))\n",
    "                        .otherwise(lit(None))\n",
    "                    )\n",
    "                elif data_type == 'double':\n",
    "                    # Para doubles: usar regex m√°s permisivo\n",
    "                    df = df.withColumn(column, \n",
    "                        when(col(column).isNull(), lit(None))\n",
    "                        .when(col(column).rlike(\"^-?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?$\"),\n",
    "                            col(column).cast(\"double\"))\n",
    "                        .otherwise(lit(None))\n",
    "                    )\n",
    "                else:\n",
    "                    # Para strings, solo limpiar espacios\n",
    "                    df = df.withColumn(column, \n",
    "                        when(col(column).isNull(), lit(None))\n",
    "                        .otherwise(trim(col(column)))\n",
    "                    )\n",
    "        return df\n",
    "    \n",
    "    # def _apply_schema_robust(self, df, schema):\n",
    "    #     \"\"\"Aplicar schema con manejo robusto de errores\"\"\"\n",
    "    #     for column, data_type in schema.items():\n",
    "    #         if column in df.columns:\n",
    "    #             if data_type in ['int', 'double']:\n",
    "    #                 # Usar try_cast para tipos num√©ricos\n",
    "    #                 df = df.withColumn(column, \n",
    "    #                     when(col(column).isNull(), lit(None))\n",
    "    #                     .otherwise(\n",
    "    #                         # Limpiar caracteres no num√©ricos primero\n",
    "    #                         when(col(column).rlike(\"^-?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?$\"),\n",
    "    #                              col(column).cast(data_type))\n",
    "    #                         .otherwise(lit(None))\n",
    "    #                     )\n",
    "    #                 )\n",
    "    #             else:\n",
    "    #                 # Para strings, solo limpiar espacios\n",
    "    #                 df = df.withColumn(column, \n",
    "    #                     when(col(column).isNull(), lit(None))\n",
    "    #                     .otherwise(trim(col(column)))\n",
    "    #                 )\n",
    "    #     return df\n",
    "    \n",
    "    def _apply_business_rules(self, datasets):\n",
    "        \"\"\"Aplicar reglas de negocio espec√≠ficas del dominio\"\"\"\n",
    "        print(\"   üéØ Aplicando reglas de negocio de tr√°fico...\")\n",
    "        \n",
    "        utd19_df = datasets['utd19']\n",
    "        \n",
    "        # Reglas de validaci√≥n para datos de tr√°fico\n",
    "        validated_utd19 = utd19_df.filter(\n",
    "            # Flow: 0 a 15,000 veh√≠culos/hora (m√°ximo te√≥rico autopista)\n",
    "            (col(\"flow\").isNull() | col(\"flow\").between(0, 15000)) &\n",
    "            \n",
    "            # Speed: 0 a 200 km/h (m√°ximo razonable urbano/autopista)\n",
    "            (col(\"speed\").isNull() | col(\"speed\").between(0, 200)) &\n",
    "            \n",
    "            # Occupancy: 0 a 1.0 (ratio de ocupaci√≥n)\n",
    "            (col(\"occ\").isNull() | col(\"occ\").between(0, 1.0)) &\n",
    "            \n",
    "            # Interval: 0 a 86400 segundos (24 horas)\n",
    "            (col(\"interval\").isNull() | col(\"interval\").between(0, 86400)) &\n",
    "            \n",
    "            # Detid debe ser positivo\n",
    "            (col(\"detid\").isNotNull() & (col(\"detid\") > 0))\n",
    "        )\n",
    "        \n",
    "        # Validaciones para detectores\n",
    "        detectors_df = datasets['detectors'] \n",
    "        validated_detectors = detectors_df.filter(\n",
    "            # Coordenadas v√°lidas\n",
    "            (col(\"lat\").isNull() | col(\"lat\").between(-90, 90)) &\n",
    "            (col(\"long\").isNull() | col(\"long\").between(-180, 180)) &\n",
    "            \n",
    "            # Lanes: 1 a 10 carriles (m√°ximo razonable)\n",
    "            (col(\"lanes\").isNull() | col(\"lanes\").between(1, 10)) &\n",
    "            \n",
    "            # Position: 0 a 1 (posici√≥n en link)\n",
    "            (col(\"pos\").isNull() | col(\"pos\").between(0, 1)) &\n",
    "            \n",
    "            # Speed limit: 10 a 200 km/h\n",
    "            (col(\"limit\").isNull() | col(\"limit\").between(10, 200))\n",
    "        )\n",
    "        \n",
    "        # Calcular m√©tricas de calidad\n",
    "        original_utd19_count = utd19_df.count()\n",
    "        validated_utd19_count = validated_utd19.count()\n",
    "        \n",
    "        validation_rate = validated_utd19_count / original_utd19_count\n",
    "        \n",
    "        print(f\"   ‚úÖ Registros v√°lidos: {validated_utd19_count:,} de {original_utd19_count:,}\")\n",
    "        print(f\"   üìä Tasa de validaci√≥n: {validation_rate:.2%}\")\n",
    "        \n",
    "        self.quality_metrics['validation_rate'] = validation_rate\n",
    "        \n",
    "        return {\n",
    "            'utd19': validated_utd19,\n",
    "            'detectors': validated_detectors,\n",
    "            'links': datasets['links']\n",
    "        }\n",
    "    \n",
    "    def _handle_missing_values(self, datasets):\n",
    "        \"\"\"Estrategia de imputaci√≥n para valores faltantes\"\"\"\n",
    "        print(\"   üîß Tratando valores faltantes...\")\n",
    "        \n",
    "        utd19_df = datasets['utd19']\n",
    "        \n",
    "        # Estrategias espec√≠ficas por variable\n",
    "        # 1. Speed: Solo 3.4% completitud - imputar con velocidad l√≠mite cuando est√© disponible\n",
    "        utd19_df = utd19_df.join(datasets['detectors'].select(\"detid\", col(\"limit\").alias(\"detector_limit\")), \"detid\", \"left\")\n",
    "        \n",
    "        utd19_imputed = utd19_df.withColumn(\n",
    "            \"speed_imputed\",\n",
    "            when(col(\"speed\").isNotNull(), col(\"speed\"))\n",
    "            .when(col(\"detector_limit\").isNotNull(), col(\"detector_limit\") * 0.8)  # 80% del l√≠mite\n",
    "            .otherwise(lit(50.0))  # Velocidad urbana promedio\n",
    "        ).withColumn(\n",
    "            \"flow_imputed\", \n",
    "            when(col(\"flow\").isNotNull(), col(\"flow\"))\n",
    "            .otherwise(lit(500))  # Flow promedio urbano\n",
    "        ).withColumn(\n",
    "            \"occ_imputed\",\n",
    "            when(col(\"occ\").isNotNull(), col(\"occ\"))\n",
    "            .otherwise(lit(0.2))  # Ocupancy promedio\n",
    "        )\n",
    "        \n",
    "        # Crear flags de imputaci√≥n para trazabilidad\n",
    "        utd19_final = utd19_imputed.withColumn(\n",
    "            \"speed_was_imputed\", col(\"speed\").isNull().cast(\"int\")\n",
    "        ).withColumn(\n",
    "            \"flow_was_imputed\", col(\"flow\").isNull().cast(\"int\") \n",
    "        ).withColumn(\n",
    "            \"occ_was_imputed\", col(\"occ\").isNull().cast(\"int\")\n",
    "        ).drop(\"speed\", \"flow\", \"occ\") \\\n",
    "         .withColumnRenamed(\"speed_imputed\", \"speed\") \\\n",
    "         .withColumnRenamed(\"flow_imputed\", \"flow\") \\\n",
    "         .withColumnRenamed(\"occ_imputed\", \"occ\")\n",
    "        \n",
    "        return {\n",
    "            'utd19': utd19_final,\n",
    "            'detectors': datasets['detectors'],\n",
    "            'links': datasets['links']\n",
    "        }\n",
    "    \n",
    "    def _detect_and_handle_outliers(self, datasets):\n",
    "        \"\"\"Detecci√≥n y tratamiento de outliers estad√≠sticos\"\"\"\n",
    "        print(\"   üìä Detectando outliers...\")\n",
    "        \n",
    "        utd19_df = datasets['utd19']\n",
    "        \n",
    "        # Calcular percentiles para detecci√≥n de outliers\n",
    "        flow_stats = utd19_df.select(\n",
    "            expr(\"percentile_approx(flow, 0.01)\").alias(\"flow_p1\"),\n",
    "            expr(\"percentile_approx(flow, 0.99)\").alias(\"flow_p99\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        speed_stats = utd19_df.select(\n",
    "            expr(\"percentile_approx(speed, 0.01)\").alias(\"speed_p1\"), \n",
    "            expr(\"percentile_approx(speed, 0.99)\").alias(\"speed_p99\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        # Aplicar winsorizaci√≥n (cap outliers a percentiles)\n",
    "        utd19_clean = utd19_df.withColumn(\n",
    "            \"flow_clean\",\n",
    "            when(col(\"flow\") < flow_stats['flow_p1'], lit(flow_stats['flow_p1']))\n",
    "            .when(col(\"flow\") > flow_stats['flow_p99'], lit(flow_stats['flow_p99']))\n",
    "            .otherwise(col(\"flow\"))\n",
    "        ).withColumn(\n",
    "            \"speed_clean\", \n",
    "            when(col(\"speed\") < speed_stats['speed_p1'], lit(speed_stats['speed_p1']))\n",
    "            .when(col(\"speed\") > speed_stats['speed_p99'], lit(speed_stats['speed_p99']))\n",
    "            .otherwise(col(\"speed\"))\n",
    "        ).drop(\"flow\", \"speed\") \\\n",
    "         .withColumnRenamed(\"flow_clean\", \"flow\") \\\n",
    "         .withColumnRenamed(\"speed_clean\", \"speed\")\n",
    "        \n",
    "        return {\n",
    "            'utd19': utd19_clean,\n",
    "            'detectors': datasets['detectors'],\n",
    "            'links': datasets['links']\n",
    "        }\n",
    "    \n",
    "    def _create_derived_features(self, datasets):\n",
    "        \"\"\"Crear features derivados para ML\"\"\"\n",
    "        print(\"   üéØ Creando features derivados...\")\n",
    "        \n",
    "        utd19_df = datasets['utd19']\n",
    "        detectors_df = datasets['detectors']\n",
    "        \n",
    "        # Join principal\n",
    "        enhanced_df = utd19_df.join(detectors_df, \"detid\", \"left\")\n",
    "        \n",
    "        # Features temporales\n",
    "        enhanced_df = enhanced_df.withColumn(\n",
    "            \"hour_of_day\", (col(\"interval\") / 3600).cast(\"int\")\n",
    "        ).withColumn(\n",
    "            \"day_of_week\", dayofweek(to_date(col(\"day\"), \"yyyy-MM-dd\"))\n",
    "        ).withColumn(\n",
    "            \"is_weekend\", when(col(\"day_of_week\").isin([1, 7]), 1).otherwise(0)\n",
    "        ).withColumn(\n",
    "            \"is_rush_hour\",\n",
    "            when(col(\"hour_of_day\").between(7, 9) | col(\"hour_of_day\").between(17, 19), 1).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        # Features de tr√°fico\n",
    "        enhanced_df = enhanced_df.withColumn(\n",
    "            \"speed_ratio\",\n",
    "            when(col(\"limit\") > 0, least(col(\"speed\") / col(\"limit\"), lit(2.0))).otherwise(1.0)\n",
    "        ).withColumn(\n",
    "            \"congestion_level\",\n",
    "            when(col(\"speed_ratio\") >= 0.8, 0)\n",
    "            .when(col(\"speed_ratio\") >= 0.6, 1) \n",
    "            .when(col(\"speed_ratio\") >= 0.4, 2)\n",
    "            .when(col(\"speed_ratio\") >= 0.2, 3)\n",
    "            .otherwise(4)\n",
    "        )\n",
    "        \n",
    "        # Features espaciales\n",
    "        enhanced_df = enhanced_df.withColumn(\n",
    "            \"near_intersection\", when(col(\"pos\") <= 0.1, 1).otherwise(0)\n",
    "        ).withColumn(\n",
    "            \"multi_lane\", when(col(\"lanes\") > 1, 1).otherwise(0)\n",
    "        ).withColumn(\n",
    "            \"has_speed_limit\", when(col(\"limit\") > 0, 1).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        # Cache para operaciones posteriores\n",
    "        enhanced_df.cache()\n",
    "        \n",
    "        return {\n",
    "            'utd19_ml_ready': enhanced_df,\n",
    "            'detectors': datasets['detectors'],\n",
    "            'links': datasets['links']\n",
    "        }\n",
    "    \n",
    "    def load_clean_data(self, clean_datasets, output_path):\n",
    "        \"\"\"Persistir datos limpios - VERSION WINDOWS COMPATIBLE\"\"\"\n",
    "        print(f\"\\nüíæ FASE 3: CARGA DE DATOS LIMPIOS\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        self.spark.conf.set(\"spark.hadoop.io.nativeio.enable\", \"false\")\n",
    "        \n",
    "        # Guardar en CSV para evitar problemas de Hadoop en Windows\n",
    "        print(\"üîÑ Guardando dataset principal...\")\n",
    "        clean_datasets['utd19_ml_ready'].coalesce(10).write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .csv(f\"{output_path}utd19clean\")\n",
    "\n",
    "        print(\"üîÑ Guardando detectores...\")\n",
    "        clean_datasets['detectors'].write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .csv(f\"{output_path}detectorsclean\")\n",
    "\n",
    "        print(\"üîÑ Guardando links...\")\n",
    "        clean_datasets['links'].write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .csv(f\"{output_path}linksclean\")\n",
    "        \n",
    "        print(f\"‚úÖ Datos guardados en: {output_path}\")\n",
    "    \n",
    "    # def load_clean_data(self, clean_datasets, output_path):\n",
    "    #     \"\"\"Persistir datos limpios en formato optimizado\"\"\"\n",
    "    #     print(f\"\\nüíæ FASE 3: CARGA DE DATOS LIMPIOS\")\n",
    "    #     print(\"-\" * 40)\n",
    "        \n",
    "    #     # Guardar en Parquet para m√°ximo rendimiento\n",
    "    #     print(\"üîÑ Guardando dataset principal...\")\n",
    "    #     clean_datasets['utd19_ml_ready'].write \\\n",
    "    #         .mode(\"overwrite\") \\\n",
    "    #         .option(\"compression\", \"snappy\") \\\n",
    "    #         .partitionBy(\"city\") \\\n",
    "    #         .parquet(f\"{output_path}utd19_clean.parquet\")\n",
    "        \n",
    "    #     print(\"üîÑ Guardando detectores...\")\n",
    "    #     clean_datasets['detectors'].write \\\n",
    "    #         .mode(\"overwrite\") \\\n",
    "    #         .parquet(f\"{output_path}detectors_clean.parquet\")\n",
    "        \n",
    "    #     print(\"üîÑ Guardando links...\")\n",
    "    #     clean_datasets['links'].write \\\n",
    "    #         .mode(\"overwrite\") \\\n",
    "    #         .parquet(f\"{output_path}links_clean.parquet\")\n",
    "        \n",
    "    #     print(f\"‚úÖ Datos guardados en: {output_path}\")\n",
    "    \n",
    "    def generate_quality_report(self):\n",
    "        \"\"\"Generar reporte de calidad de datos\"\"\"\n",
    "        print(f\"\\nüìã REPORTE DE CALIDAD DE DATOS\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        print(f\"üìä Registros originales: {self.quality_metrics['raw_counts']['utd19']:,}\")\n",
    "        print(f\"‚úÖ Tasa de validaci√≥n: {self.quality_metrics['validation_rate']:.2%}\")\n",
    "        \n",
    "        # Calcular registros finales estimados\n",
    "        final_estimated = int(self.quality_metrics['raw_counts']['utd19'] * self.quality_metrics['validation_rate'])\n",
    "        print(f\"üéØ Registros limpios estimados: {final_estimated:,}\")\n",
    "        \n",
    "        reduction_rate = 1 - self.quality_metrics['validation_rate']\n",
    "        print(f\"üìâ Reducci√≥n por limpieza: {reduction_rate:.2%}\")\n",
    "        \n",
    "        if self.quality_metrics['validation_rate'] > 0.8:\n",
    "            print(\"‚úÖ CALIDAD: Excelente (>80% datos v√°lidos)\")\n",
    "        elif self.quality_metrics['validation_rate'] > 0.6:\n",
    "            print(\"‚ö†Ô∏è CALIDAD: Buena (>60% datos v√°lidos)\")\n",
    "        else:\n",
    "            print(\"üî¥ CALIDAD: Requiere atenci√≥n (<60% datos v√°lidos)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ed5c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatioTemporalLSTM(tf.keras.Model):\n",
    "    def __init__(self, num_detectors, sequence_length=12, feature_dim=10):\n",
    "        super().__init__()\n",
    "        self.num_detectors = num_detectors\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Embedding espacial para detectores\n",
    "        self.detector_embedding = layers.Embedding(\n",
    "            input_dim=num_detectors + 1,  # +1 para detectores desconocidos\n",
    "            output_dim=32,\n",
    "            mask_zero=True,\n",
    "            name=\"detector_embedding\"\n",
    "        )\n",
    "        \n",
    "        # Capas LSTM para modelado temporal\n",
    "        self.lstm1 = layers.LSTM(\n",
    "            128, \n",
    "            return_sequences=True, \n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2,\n",
    "            name=\"lstm_1\"\n",
    "        )\n",
    "        self.lstm2 = layers.LSTM(\n",
    "            64, \n",
    "            return_sequences=False,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2,\n",
    "            name=\"lstm_2\"\n",
    "        )\n",
    "        \n",
    "        # Capas densas para procesamiento\n",
    "        self.dense1 = layers.Dense(64, activation='relu', name=\"dense_1\")\n",
    "        self.dropout1 = layers.Dropout(0.3)\n",
    "        self.dense2 = layers.Dense(32, activation='relu', name=\"dense_2\")\n",
    "        self.dropout2 = layers.Dropout(0.2)\n",
    "        \n",
    "        # Salidas m√∫ltiples\n",
    "        self.flow_output = layers.Dense(1, activation='linear', name='flow_prediction')\n",
    "        self.congestion_output = layers.Dense(5, activation='softmax', name='congestion_prediction')\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs: [temporal_features, detector_ids]\n",
    "        temporal_features, detector_ids = inputs\n",
    "        \n",
    "        # Procesar embeddings espaciales\n",
    "        detector_emb = self.detector_embedding(detector_ids)\n",
    "        \n",
    "        # Combinar features temporales con embeddings espaciales\n",
    "        # temporal_features: [batch, sequence, features]\n",
    "        # detector_emb: [batch, embedding_dim]\n",
    "        detector_emb_expanded = tf.expand_dims(detector_emb, axis=1)\n",
    "        detector_emb_tiled = tf.tile(detector_emb_expanded, [1, self.sequence_length, 1])\n",
    "        \n",
    "        combined_features = tf.concat([temporal_features, detector_emb_tiled], axis=-1)\n",
    "        \n",
    "        # Procesamiento LSTM\n",
    "        lstm_out = self.lstm1(combined_features, training=training)\n",
    "        lstm_out = self.lstm2(lstm_out, training=training)\n",
    "        \n",
    "        # Capas densas\n",
    "        dense_out = self.dense1(lstm_out)\n",
    "        dense_out = self.dropout1(dense_out, training=training)\n",
    "        dense_out = self.dense2(dense_out)\n",
    "        dense_out = self.dropout2(dense_out, training=training)\n",
    "        \n",
    "        # Predicciones\n",
    "        flow_pred = self.flow_output(dense_out)\n",
    "        congestion_pred = self.congestion_output(dense_out)\n",
    "        \n",
    "        return {\n",
    "            'flow_prediction': flow_pred,\n",
    "            'congestion_prediction': congestion_pred\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "584da004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PASO 4: PREPARACI√ìN DE DATOS PARA DEEP LEARNING =====\n",
    "def prepare_sequences_for_lstm(ml_df_pandas, sequence_length=12):\n",
    "    \"\"\"Crear secuencias temporales para LSTM\"\"\"\n",
    "    print(f\"\\nüîÑ Creando secuencias temporales (longitud: {sequence_length})\")\n",
    "    \n",
    "    # Features para secuencias temporales (variables exactas UTD19)\n",
    "    temporal_features = [\n",
    "        'flow', 'speed', 'occ', 'hour_of_day', 'is_rush_hour',\n",
    "        'flow_lag_1', 'speed_trend', 'lanes', 'pos', 'near_intersection',\n",
    "        'length', 'has_speed_limit'  # Variables adicionales del dataset real\n",
    "    ]\n",
    "    \n",
    "    # Normalizar features temporales\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    ml_df_pandas[temporal_features] = scaler.fit_transform(ml_df_pandas[temporal_features])\n",
    "    \n",
    "    # Crear mapeo de detector IDs\n",
    "    unique_detectors = ml_df_pandas['detid'].unique()\n",
    "    detector_to_id = {det: i+1 for i, det in enumerate(unique_detectors)}\n",
    "    ml_df_pandas['detector_id'] = ml_df_pandas['detid'].map(detector_to_id)\n",
    "    \n",
    "    # Crear secuencias por detector\n",
    "    X_temporal, X_detector, y_flow, y_congestion = [], [], [], []\n",
    "    \n",
    "    for detector in unique_detectors:\n",
    "        detector_data = ml_df_pandas[ml_df_pandas['detid'] == detector].sort_values(['day', 'interval'])\n",
    "        \n",
    "        if len(detector_data) < sequence_length:\n",
    "            continue\n",
    "            \n",
    "        for i in range(sequence_length, len(detector_data)):\n",
    "            # Secuencia temporal\n",
    "            sequence = detector_data.iloc[i-sequence_length:i][temporal_features].values\n",
    "            X_temporal.append(sequence)\n",
    "            \n",
    "            # ID del detector\n",
    "            X_detector.append(detector_to_id[detector])\n",
    "            \n",
    "            # Targets\n",
    "            target_row = detector_data.iloc[i]\n",
    "            y_flow.append(target_row['flow'])\n",
    "            y_congestion.append(target_row['congestion_level'])\n",
    "    \n",
    "    print(f\"‚úÖ Secuencias creadas: {len(X_temporal):,}\")\n",
    "    \n",
    "    return (np.array(X_temporal), np.array(X_detector), \n",
    "            np.array(y_flow), np.array(y_congestion), \n",
    "            scaler, detector_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "662bf8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficManagementMetrics:\n",
    "    def __init__(self):\n",
    "        self.baseline_metrics = {}\n",
    "        self.optimized_metrics = {}\n",
    "    \n",
    "    def calculate_prediction_accuracy(self, model, X_test, y_true_flow, y_true_congestion):\n",
    "        \"\"\"M√©trica 1: Prediction accuracy\"\"\"\n",
    "        predictions = model.predict([X_test[0], X_test[1]])\n",
    "        \n",
    "        # Accuracy para predicci√≥n de congesti√≥n\n",
    "        from sklearn.metrics import accuracy_score, classification_report\n",
    "        y_pred_congestion = np.argmax(predictions['congestion_prediction'], axis=1)\n",
    "        congestion_accuracy = accuracy_score(y_true_congestion, y_pred_congestion)\n",
    "        \n",
    "        # MAE para predicci√≥n de flujo\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        flow_mae = mean_absolute_error(y_true_flow, predictions['flow_prediction'])\n",
    "        \n",
    "        print(f\"üìä M√âTRICA 1 - PREDICTION ACCURACY:\")\n",
    "        print(f\"  üö¶ Congesti√≥n Accuracy: {congestion_accuracy:.3f}\")\n",
    "        print(f\"  üöó Flujo MAE: {flow_mae:.2f} veh/h\")\n",
    "        print(f\"  üìà Score General: {(congestion_accuracy * 100):.1f}%\")\n",
    "\n",
    "        # Agregar al final del m√©todo calculate_prediction_accuracy:\n",
    "        \n",
    "        # Gr√°fico de accuracy por nivel de congesti√≥n\n",
    "        from sklearn.metrics import classification_report\n",
    "        report = classification_report(y_true_congestion, y_pred_congestion, \n",
    "                                     target_names=['Free', 'Light', 'Moderate', 'Heavy', 'Severe'],\n",
    "                                     output_dict=True)\n",
    "        \n",
    "        # Visualizar accuracy por clase\n",
    "        classes = ['Free', 'Light', 'Moderate', 'Heavy', 'Severe']\n",
    "        f1_scores = [report[cls]['f1-score'] for cls in classes]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(classes, f1_scores, color=['green', 'yellow', 'orange', 'red', 'darkred'])\n",
    "        plt.title('F1-Score por Nivel de Congesti√≥n')\n",
    "        plt.xlabel('Nivel de Congesti√≥n')\n",
    "        plt.ylabel('F1-Score')\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # Agregar valores en las barras\n",
    "        for bar, score in zip(bars, f1_scores):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{score:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig('f1_scores_by_congestion_level.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'congestion_accuracy': congestion_accuracy,\n",
    "            'flow_mae': flow_mae,\n",
    "            'overall_score': congestion_accuracy\n",
    "        }\n",
    "    \n",
    "    def calculate_travel_time_reduction(self, baseline_data, optimized_data):\n",
    "        \"\"\"M√©trica 2: Reduction in average travel time\"\"\"\n",
    "        # Calcular tiempo promedio de viaje basado en velocidad y distancia\n",
    "        def calculate_avg_travel_time(data):\n",
    "            # Usar velocidad promedio para estimar tiempo de viaje\n",
    "            avg_speed = data['speed'].mean()\n",
    "            # Asumir distancia promedio de 5 km para viajes urbanos\n",
    "            avg_distance_km = 5.0\n",
    "            avg_travel_time_hours = avg_distance_km / avg_speed if avg_speed > 0 else 1.0\n",
    "            return avg_travel_time_hours * 60  # Convertir a minutos\n",
    "        \n",
    "        baseline_time = calculate_avg_travel_time(baseline_data)\n",
    "        optimized_time = calculate_avg_travel_time(optimized_data)\n",
    "        \n",
    "        time_reduction = baseline_time - optimized_time\n",
    "        reduction_percentage = (time_reduction / baseline_time) * 100\n",
    "        \n",
    "        print(f\"üìä M√âTRICA 2 - TRAVEL TIME REDUCTION:\")\n",
    "        print(f\"  ‚è±Ô∏è Tiempo baseline: {baseline_time:.1f} min\")\n",
    "        print(f\"  ‚ö° Tiempo optimizado: {optimized_time:.1f} min\")\n",
    "        print(f\"  üìâ Reducci√≥n: {time_reduction:.1f} min ({reduction_percentage:.1f}%)\")\n",
    "        \n",
    "        return {\n",
    "            'baseline_time_min': baseline_time,\n",
    "            'optimized_time_min': optimized_time,\n",
    "            'reduction_min': time_reduction,\n",
    "            'reduction_percentage': reduction_percentage\n",
    "        }\n",
    "    \n",
    "    def calculate_fuel_savings(self, baseline_data, optimized_data):\n",
    "        \"\"\"M√©trica 3: Fuel savings\"\"\"\n",
    "        # F√≥rmula simplificada: consumo inversamente proporcional a velocidad\n",
    "        def calculate_fuel_consumption(data):\n",
    "            # Consumo base: 8L/100km a 50 km/h\n",
    "            base_consumption = 8.0  # L/100km\n",
    "            reference_speed = 50.0  # km/h\n",
    "            \n",
    "            # Factor de correcci√≥n por velocidad (m√°s lento = m√°s consumo)\n",
    "            speed_factor = data['speed'].apply(\n",
    "                lambda s: max(1.0, reference_speed / max(s, 10))  # Evitar divisi√≥n por 0\n",
    "            )\n",
    "            \n",
    "            # Consumo por km\n",
    "            consumption_per_km = (base_consumption / 100) * speed_factor\n",
    "            \n",
    "            # Asumir 5 km promedio de distancia\n",
    "            total_consumption = consumption_per_km * 5.0\n",
    "            \n",
    "            return total_consumption.mean()\n",
    "        \n",
    "        baseline_fuel = calculate_fuel_consumption(baseline_data)\n",
    "        optimized_fuel = calculate_fuel_consumption(optimized_data)\n",
    "        \n",
    "        fuel_savings = baseline_fuel - optimized_fuel\n",
    "        savings_percentage = (fuel_savings / baseline_fuel) * 100\n",
    "        \n",
    "        # Convertir a ahorro monetario (precio combustible ~$1.50/L)\n",
    "        fuel_price_per_liter = 1.50\n",
    "        monetary_savings = fuel_savings * fuel_price_per_liter\n",
    "        \n",
    "        print(f\"üìä M√âTRICA 3 - FUEL SAVINGS:\")\n",
    "        print(f\"  ‚õΩ Consumo baseline: {baseline_fuel:.2f} L/viaje\")\n",
    "        print(f\"  üå± Consumo optimizado: {optimized_fuel:.2f} L/viaje\")\n",
    "        print(f\"  üí∞ Ahorro: {fuel_savings:.2f} L/viaje ({savings_percentage:.1f}%)\")\n",
    "        print(f\"  üíµ Ahorro monetario: ${monetary_savings:.2f}/viaje\")\n",
    "        \n",
    "        return {\n",
    "            'baseline_fuel_L': baseline_fuel,\n",
    "            'optimized_fuel_L': optimized_fuel,\n",
    "            'savings_L': fuel_savings,\n",
    "            'savings_percentage': savings_percentage,\n",
    "            'monetary_savings_USD': monetary_savings\n",
    "        }\n",
    "    \n",
    "    def simulate_user_satisfaction(self, travel_time_reduction, fuel_savings):\n",
    "        \"\"\"M√©trica 4: User satisfaction (simulada)\"\"\"\n",
    "        # Simulaci√≥n basada en mejoras objetivas\n",
    "        time_satisfaction = min(100, max(0, 50 + (travel_time_reduction['reduction_percentage'] * 2)))\n",
    "        fuel_satisfaction = min(100, max(0, 50 + (fuel_savings['savings_percentage'] * 3)))\n",
    "        \n",
    "        # Factores adicionales simulados\n",
    "        reliability_score = 85  # Consistencia del sistema\n",
    "        ease_of_use_score = 78  # Facilidad de uso\n",
    "        \n",
    "        overall_satisfaction = (time_satisfaction + fuel_satisfaction + \n",
    "                              reliability_score + ease_of_use_score) / 4\n",
    "        \n",
    "        print(f\"üìä M√âTRICA 4 - USER SATISFACTION:\")\n",
    "        print(f\"  ‚è±Ô∏è Satisfacci√≥n tiempo: {time_satisfaction:.1f}/100\")\n",
    "        print(f\"  ‚õΩ Satisfacci√≥n combustible: {fuel_satisfaction:.1f}/100\")\n",
    "        print(f\"  üîÑ Confiabilidad: {reliability_score}/100\")\n",
    "        print(f\"  üì± Facilidad uso: {ease_of_use_score}/100\")\n",
    "        print(f\"  üéØ Satisfacci√≥n general: {overall_satisfaction:.1f}/100\")\n",
    "        \n",
    "        return {\n",
    "            'time_satisfaction': time_satisfaction,\n",
    "            'fuel_satisfaction': fuel_satisfaction,\n",
    "            'reliability_score': reliability_score,\n",
    "            'ease_of_use_score': ease_of_use_score,\n",
    "            'overall_satisfaction': overall_satisfaction\n",
    "        }\n",
    "    \n",
    "    def generate_final_report(self, prediction_metrics, travel_time_metrics, \n",
    "                            fuel_metrics, satisfaction_metrics):\n",
    "        \"\"\"Generar reporte final con todas las m√©tricas del caso de estudio\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìã REPORTE FINAL - CASE STUDY METRICS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nüéØ RESUMEN EJECUTIVO:\")\n",
    "        print(f\"  ‚úÖ Precisi√≥n de predicci√≥n: {prediction_metrics['overall_score']:.1%}\")\n",
    "        print(f\"  ‚è±Ô∏è Reducci√≥n tiempo viaje: {travel_time_metrics['reduction_percentage']:.1f}%\")\n",
    "        print(f\"  ‚õΩ Ahorro combustible: {fuel_metrics['savings_percentage']:.1f}%\")\n",
    "        print(f\"  üòä Satisfacci√≥n usuario: {satisfaction_metrics['overall_satisfaction']:.1f}/100\")\n",
    "        \n",
    "        # Impacto econ√≥mico\n",
    "        daily_trips = 10000  # Estimaci√≥n para una ciudad\n",
    "        daily_savings = fuel_metrics['monetary_savings_USD'] * daily_trips\n",
    "        annual_savings = daily_savings * 365\n",
    "        \n",
    "        print(f\"\\nüí∞ IMPACTO ECON√ìMICO ESTIMADO:\")\n",
    "        print(f\"  üìä Viajes diarios estimados: {daily_trips:,}\")\n",
    "        print(f\"  üíµ Ahorro diario: ${daily_savings:,.2f}\")\n",
    "        print(f\"  üè¶ Ahorro anual: ${annual_savings:,.2f}\")\n",
    "        \n",
    "        # ROI del sistema\n",
    "        system_cost = 5000000  # $5M estimado\n",
    "        roi_years = system_cost / annual_savings\n",
    "        \n",
    "        print(f\"  üìà ROI del sistema: {roi_years:.1f} a√±os\")\n",
    "        \n",
    "        return {\n",
    "            'summary': {\n",
    "                'prediction_accuracy': prediction_metrics['overall_score'],\n",
    "                'travel_time_reduction_pct': travel_time_metrics['reduction_percentage'],\n",
    "                'fuel_savings_pct': fuel_metrics['savings_percentage'],\n",
    "                'user_satisfaction': satisfaction_metrics['overall_satisfaction']\n",
    "            },\n",
    "            'economic_impact': {\n",
    "                'daily_savings_USD': daily_savings,\n",
    "                'annual_savings_USD': annual_savings,\n",
    "                'roi_years': roi_years\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e8b658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== VISUALIZACI√ìN DE RESULTADOS =====\n",
    "class TrafficVisualization:\n",
    "    def __init__(self):\n",
    "        plt.style.use('default')\n",
    "        sns.set_palette(\"husl\")\n",
    "    \n",
    "    def plot_data_distribution(self, ml_df_pandas):\n",
    "        \"\"\"Gr√°ficos de distribuci√≥n de datos\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('UTD19 Dataset - Distribuci√≥n de Variables', fontsize=16)\n",
    "        \n",
    "        # Distribuci√≥n de flujo\n",
    "        axes[0,0].hist(ml_df_pandas['flow'].dropna(), bins=50, alpha=0.7)\n",
    "        axes[0,0].set_title('Distribuci√≥n de Flujo (veh/h)')\n",
    "        axes[0,0].set_xlabel('Flujo')\n",
    "        axes[0,0].set_ylabel('Frecuencia')\n",
    "        \n",
    "        # Distribuci√≥n de velocidad\n",
    "        axes[0,1].hist(ml_df_pandas['speed'].dropna(), bins=50, alpha=0.7, color='orange')\n",
    "        axes[0,1].set_title('Distribuci√≥n de Velocidad (km/h)')\n",
    "        axes[0,1].set_xlabel('Velocidad')\n",
    "        \n",
    "        # Distribuci√≥n de ocupancy\n",
    "        axes[0,2].hist(ml_df_pandas['occ'].dropna(), bins=50, alpha=0.7, color='green')\n",
    "        axes[0,2].set_title('Distribuci√≥n de Ocupancy')\n",
    "        axes[0,2].set_xlabel('Ocupancy')\n",
    "        \n",
    "        # Congesti√≥n por hora del d√≠a\n",
    "        congestion_hour = ml_df_pandas.groupby('hour_of_day')['congestion_level'].mean()\n",
    "        axes[1,0].plot(congestion_hour.index, congestion_hour.values, marker='o')\n",
    "        axes[1,0].set_title('Congesti√≥n Promedio por Hora')\n",
    "        axes[1,0].set_xlabel('Hora del d√≠a')\n",
    "        axes[1,0].set_ylabel('Nivel de congesti√≥n')\n",
    "        \n",
    "        # Distribuci√≥n por ciudad\n",
    "        city_counts = ml_df_pandas['city'].value_counts().head(10)\n",
    "        axes[1,1].bar(range(len(city_counts)), city_counts.values)\n",
    "        axes[1,1].set_title('Top 10 Ciudades por Registros')\n",
    "        axes[1,1].set_xticks(range(len(city_counts)))\n",
    "        axes[1,1].set_xticklabels(city_counts.index, rotation=45)\n",
    "        \n",
    "        # Heatmap de correlaci√≥n\n",
    "        corr_vars = ['flow', 'speed', 'occ', 'hour_of_day', 'congestion_level']\n",
    "        corr_matrix = ml_df_pandas[corr_vars].corr()\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1,2])\n",
    "        axes[1,2].set_title('Matriz de Correlaci√≥n')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('utd19_data_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_training_history(self, history):\n",
    "        \"\"\"Gr√°fico de entrenamiento del modelo\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Historial de Entrenamiento del Modelo', fontsize=16)\n",
    "        \n",
    "        # Loss total\n",
    "        axes[0,0].plot(history.history['loss'], label='Train Loss')\n",
    "        axes[0,0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "        axes[0,0].set_title('Loss Total')\n",
    "        axes[0,0].set_xlabel('√âpoca')\n",
    "        axes[0,0].set_ylabel('Loss')\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].grid(True)\n",
    "        \n",
    "        # Flow prediction MAE\n",
    "        axes[0,1].plot(history.history['flow_prediction_mae'], label='Train MAE')\n",
    "        axes[0,1].plot(history.history['val_flow_prediction_mae'], label='Val MAE')\n",
    "        axes[0,1].set_title('Flow Prediction - MAE')\n",
    "        axes[0,1].set_xlabel('√âpoca')\n",
    "        axes[0,1].set_ylabel('MAE')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True)\n",
    "        \n",
    "        # Congestion accuracy\n",
    "        axes[1,0].plot(history.history['congestion_prediction_accuracy'], label='Train Acc')\n",
    "        axes[1,0].plot(history.history['val_congestion_prediction_accuracy'], label='Val Acc')\n",
    "        axes[1,0].set_title('Congestion Prediction - Accuracy')\n",
    "        axes[1,0].set_xlabel('√âpoca')\n",
    "        axes[1,0].set_ylabel('Accuracy')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True)\n",
    "        \n",
    "        # Learning curves combined\n",
    "        axes[1,1].plot(history.history['loss'], label='Train Loss', alpha=0.7)\n",
    "        axes[1,1].plot(history.history['val_loss'], label='Val Loss', alpha=0.7)\n",
    "        ax2 = axes[1,1].twinx()\n",
    "        ax2.plot(history.history['congestion_prediction_accuracy'], \n",
    "                label='Train Acc', color='green', alpha=0.7)\n",
    "        ax2.plot(history.history['val_congestion_prediction_accuracy'], \n",
    "                label='Val Acc', color='red', alpha=0.7)\n",
    "        axes[1,1].set_title('Loss vs Accuracy')\n",
    "        axes[1,1].set_xlabel('√âpoca')\n",
    "        axes[1,1].set_ylabel('Loss')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrix(self, y_true, y_pred):\n",
    "        \"\"\"Matriz de confusi√≥n para clasificaci√≥n de congesti√≥n\"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=['Free', 'Light', 'Moderate', 'Heavy', 'Severe'],\n",
    "                   yticklabels=['Free', 'Light', 'Moderate', 'Heavy', 'Severe'])\n",
    "        plt.title('Matriz de Confusi√≥n - Predicci√≥n de Congesti√≥n', fontsize=14)\n",
    "        plt.xlabel('Predicci√≥n')\n",
    "        plt.ylabel('Valor Real')\n",
    "        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Classification report\n",
    "        print(\"\\nüìä REPORTE DE CLASIFICACI√ìN:\")\n",
    "        print(classification_report(y_true, y_pred, \n",
    "                                  target_names=['Free', 'Light', 'Moderate', 'Heavy', 'Severe']))\n",
    "    \n",
    "    def plot_predictions_vs_actual(self, y_true_flow, y_pred_flow, sample_size=1000):\n",
    "        \"\"\"Scatter plot de predicciones vs valores reales\"\"\"\n",
    "        # Tomar muestra para visualizaci√≥n\n",
    "        indices = np.random.choice(len(y_true_flow), min(sample_size, len(y_true_flow)), replace=False)\n",
    "        y_true_sample = y_true_flow[indices]\n",
    "        y_pred_sample = y_pred_flow[indices].flatten()\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Scatter plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(y_true_sample, y_pred_sample, alpha=0.5)\n",
    "        plt.plot([y_true_sample.min(), y_true_sample.max()], \n",
    "                [y_true_sample.min(), y_true_sample.max()], 'r--', lw=2)\n",
    "        plt.xlabel('Flujo Real (veh/h)')\n",
    "        plt.ylabel('Flujo Predicho (veh/h)')\n",
    "        plt.title('Predicciones vs Valores Reales - Flujo')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Residual plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        residuals = y_true_sample - y_pred_sample\n",
    "        plt.scatter(y_pred_sample, residuals, alpha=0.5)\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.xlabel('Flujo Predicho (veh/h)')\n",
    "        plt.ylabel('Residuales')\n",
    "        plt.title('Gr√°fico de Residuales')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('predictions_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5301777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_utd19_data(spark, data_path=\"./data/processed/\"):\n",
    "    \"\"\"Funci√≥n helper para cargar datos ya procesados\"\"\"\n",
    "    print(\"üì• Cargando datos UTD19 limpios...\")\n",
    "\n",
    "    utd19_clean = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(f\"{data_path}utd19clean\")\n",
    "    detectors_clean = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(f\"{data_path}detectorsclean\")\n",
    "    links_clean = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(f\"{data_path}linksclean\")\n",
    "\n",
    "    print(f\"‚úÖ Datos cargados: {utd19_clean.count():,} registros\")\n",
    "    \n",
    "    return utd19_clean, detectors_clean, links_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b83413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FUNCI√ìN PRINCIPAL DE EJECUCI√ìN =====\n",
    "def main_implementation():\n",
    "    \"\"\"Funci√≥n principal para ejecutar todo el pipeline\"\"\"\n",
    "    print(\"üöÄ INICIANDO IMPLEMENTACI√ìN UTD19 BIG DATA + DEEP LEARNING\")\n",
    "    print(\"üéØ ENFOQUE: Real-time traffic management system\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Setup Spark\n",
    "    spark = setup_spark_session()\n",
    "\n",
    "    # 2. ETL Pipeline - Verificar si existen datos limpios\n",
    "    try:\n",
    "        utd19_clean, detectors_clean, links_clean = load_clean_utd19_data(spark)\n",
    "        print(\"‚úÖ Usando datos previamente procesados por ETL\")\n",
    "        ml_df = utd19_clean  # Los datos ya est√°n listos para ML\n",
    "    except:\n",
    "        print(\"‚ö° Datos limpios no encontrados. Ejecutando ETL completo...\")\n",
    "        \n",
    "        # Ejecutar ETL completo\n",
    "        etl_pipeline = UTD19_ETL_Pipeline(spark)\n",
    "        clean_datasets = etl_pipeline.run_complete_etl()\n",
    "        \n",
    "        # Usar datos procesados\n",
    "        ml_df = clean_datasets['utd19_ml_ready']\n",
    "        detectors_clean = clean_datasets['detectors']\n",
    "        links_clean = clean_datasets['links']\n",
    "    \n",
    "    # # 2. Cargar datasets\n",
    "    # loader = UTD19BigDataLoader(spark)\n",
    "    # detectors_df, links_df, utd19_df = loader.load_utd19_datasets()\n",
    "    \n",
    "    # # 3. An√°lisis inicial\n",
    "    # loader.analyze_data_distribution(utd19_df, detectors_df)\n",
    "    \n",
    "    # # 4. Feature engineering\n",
    "    # feature_eng = BigDataFeatureEngineering(spark)\n",
    "    # enhanced_df = feature_eng.create_enhanced_features(utd19_df, detectors_df)\n",
    "    # ml_df = feature_eng.prepare_ml_dataset(enhanced_df)\n",
    "    \n",
    "    # 5. Convertir a Pandas para Deep Learning (muestra)\n",
    "    print(\"\\nüì• Convirtiendo muestra para Deep Learning...\")\n",
    "    sample_size = 1000000  # 1M registros para empezar\n",
    "    ml_df_pandas = ml_df.limit(sample_size).toPandas()\n",
    "    print(f\"‚úÖ Muestra convertida: {len(ml_df_pandas):,} registros\")\n",
    "    \n",
    "    # 6. Preparar secuencias para LSTM\n",
    "    X_temporal, X_detector, y_flow, y_congestion, scaler, detector_mapping = \\\n",
    "        prepare_sequences_for_lstm(ml_df_pandas)\n",
    "    \n",
    "    # 7. Crear y compilar modelo\n",
    "    print(\"\\nüß† Creando modelo Deep Learning...\")\n",
    "    model = SpatioTemporalLSTM(\n",
    "        num_detectors=len(detector_mapping),\n",
    "        sequence_length=12,\n",
    "        feature_dim=10\n",
    "    )\n",
    "    \n",
    "    # Compilar con m√∫ltiples salidas\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={\n",
    "            'flow_prediction': 'mse',\n",
    "            'congestion_prediction': 'sparse_categorical_crossentropy'\n",
    "        },\n",
    "        loss_weights={\n",
    "            'flow_prediction': 1.0,\n",
    "            'congestion_prediction': 2.0\n",
    "        },\n",
    "        metrics={\n",
    "            'flow_prediction': ['mae'],\n",
    "            'congestion_prediction': ['accuracy']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Modelo creado y compilado\")\n",
    "    print(f\"üìä Datos preparados para entrenamiento: {X_temporal.shape}\")\n",
    "    \n",
    "    # 8. Divisi√≥n train/test\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_temp_train, X_temp_test, X_det_train, X_det_test, \\\n",
    "    y_flow_train, y_flow_test, y_cong_train, y_cong_test = train_test_split(\n",
    "        X_temporal, X_detector, y_flow, y_congestion,\n",
    "        test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"üéØ Train set: {X_temp_train.shape[0]:,} secuencias\")\n",
    "    print(f\"üéØ Test set: {X_temp_test.shape[0]:,} secuencias\")\n",
    "    \n",
    "    # 9. Entrenar modelo\n",
    "    print(\"\\nüöÄ INICIANDO ENTRENAMIENTO...\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        x=[X_temp_train, X_det_train],\n",
    "        y={\n",
    "            'flow_prediction': y_flow_train,\n",
    "            'congestion_prediction': y_cong_train\n",
    "        },\n",
    "        validation_data=(\n",
    "            [X_temp_test, X_det_test],\n",
    "            {\n",
    "                'flow_prediction': y_flow_test,\n",
    "                'congestion_prediction': y_cong_test\n",
    "            }\n",
    "        ),\n",
    "        epochs=20,\n",
    "        batch_size=128,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ ENTRENAMIENTO COMPLETADO\")\n",
    "\n",
    "    print(\"\\nüìä GENERANDO VISUALIZACIONES...\")\n",
    "    viz = TrafficVisualization()\n",
    "    \n",
    "    # 1. Distribuci√≥n de datos\n",
    "    viz.plot_data_distribution(ml_df_pandas)\n",
    "    \n",
    "    # 2. Historial de entrenamiento\n",
    "    viz.plot_training_history(history)\n",
    "    \n",
    "    # 3. Predicciones del modelo\n",
    "    print(\"üîÆ Generando predicciones para evaluaci√≥n...\")\n",
    "    predictions = model.predict([X_temp_test, X_det_test])\n",
    "    \n",
    "    # 4. Matriz de confusi√≥n\n",
    "    y_pred_congestion = np.argmax(predictions['congestion_prediction'], axis=1)\n",
    "    viz.plot_confusion_matrix(y_cong_test, y_pred_congestion)\n",
    "    \n",
    "    # 5. Predicciones vs valores reales (flujo)\n",
    "    y_pred_flow = predictions['flow_prediction']\n",
    "    viz.plot_predictions_vs_actual(y_flow_test, y_pred_flow)\n",
    "    \n",
    "    # 10. EVALUACI√ìN CON M√âTRICAS DEL CASO DE ESTUDIO\n",
    "    print(\"\\nüìä EVALUANDO CON M√âTRICAS ESPEC√çFICAS DEL CASO...\")\n",
    "    \n",
    "    # Inicializar sistema de m√©tricas\n",
    "    metrics_system = TrafficManagementMetrics()\n",
    "    \n",
    "    # M√©trica 1: Prediction Accuracy\n",
    "    prediction_metrics = metrics_system.calculate_prediction_accuracy(\n",
    "        model, [X_temp_test, X_det_test], y_flow_test, y_cong_test\n",
    "    )\n",
    "    \n",
    "    # Preparar datos para m√©tricas 2 y 3 (baseline vs optimizado)\n",
    "    # Simular datos baseline (sin optimizaci√≥n) y optimizados\n",
    "    test_data_baseline = ml_df_pandas.sample(n=10000, random_state=42)\n",
    "    \n",
    "    # Simular mejoras del sistema (velocidades 15% mayores en promedio)\n",
    "    test_data_optimized = test_data_baseline.copy()\n",
    "    test_data_optimized['speed'] = test_data_optimized['speed'] * 1.15\n",
    "    \n",
    "    # M√©trica 2: Travel Time Reduction\n",
    "    travel_time_metrics = metrics_system.calculate_travel_time_reduction(\n",
    "        test_data_baseline, test_data_optimized\n",
    "    )\n",
    "    \n",
    "    # M√©trica 3: Fuel Savings\n",
    "    fuel_metrics = metrics_system.calculate_fuel_savings(\n",
    "        test_data_baseline, test_data_optimized\n",
    "    )\n",
    "    \n",
    "    # M√©trica 4: User Satisfaction\n",
    "    satisfaction_metrics = metrics_system.simulate_user_satisfaction(\n",
    "        travel_time_metrics, fuel_metrics\n",
    "    )\n",
    "    \n",
    "    # Reporte final completo\n",
    "    final_report = metrics_system.generate_final_report(\n",
    "        prediction_metrics, travel_time_metrics, \n",
    "        fuel_metrics, satisfaction_metrics\n",
    "    )\n",
    "    \n",
    "    # Guardar modelo y m√©tricas\n",
    "    model.save_weights('utd19_spatiotemporal_model.h5')\n",
    "    print(\"üíæ Modelo guardado: utd19_spatiotemporal_model.h5\")\n",
    "    \n",
    "    # Guardar m√©tricas en JSON\n",
    "    import json\n",
    "    with open('case_study_metrics.json', 'w') as f:\n",
    "        json.dump(final_report, f, indent=2)\n",
    "    print(\"üìÑ M√©tricas guardadas: case_study_metrics.json\")\n",
    "    \n",
    "    return model, history, scaler, detector_mapping, final_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a8922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ INICIANDO IMPLEMENTACI√ìN UTD19 BIG DATA + DEEP LEARNING\n",
      "üéØ ENFOQUE: Real-time traffic management system\n",
      "============================================================\n",
      "‚úÖ Spark configurado - Versi√≥n: 4.0.1\n",
      "üìä Configuraci√≥n: 8 executors, 16GB memoria cada uno\n",
      "üì• Cargando datos UTD19 limpios...\n",
      "‚ö° Datos limpios no encontrados. Ejecutando ETL completo...\n",
      "============================================================\n",
      "üöÄ INICIANDO ETL PIPELINE COMPLETO UTD19\n",
      "============================================================\n",
      "\n",
      "üì• FASE 1: EXTRACCI√ìN DE DATOS\n",
      "----------------------------------------\n",
      "üîÑ Cargando detectores...\n",
      "üîÑ Cargando links...\n",
      "üîÑ Cargando UTD19 principal (puede tomar varios minutos)...\n",
      "‚úÖ Detectores: 23,626 registros\n",
      "‚úÖ Links: 140,858 registros\n",
      "‚úÖ UTD19: 134,380,371 registros\n",
      "\n",
      "üîß FASE 2: TRANSFORMACI√ìN DE DATOS\n",
      "----------------------------------------\n",
      "üìã Paso 1: Enforcement de tipos de datos...\n",
      "   üîÑ Convirtiendo tipos de datos...\n",
      "‚úÖ Paso 2: Aplicaci√≥n de reglas de negocio...\n",
      "   üéØ Aplicando reglas de negocio de tr√°fico...\n",
      "   ‚úÖ Registros v√°lidos: 17,115,417 de 134,380,371\n",
      "   üìä Tasa de validaci√≥n: 12.74%\n",
      "üîß Paso 3: Tratamiento de valores faltantes...\n",
      "   üîß Tratando valores faltantes...\n",
      "üìä Paso 4: Detecci√≥n de outliers...\n",
      "   üìä Detectando outliers...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, history, scaler, detector_mapping, metrics_report = main_implementation()\n",
    "    print(\"\\nüéâ IMPLEMENTACI√ìN UTD19 COMPLETADA!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"üìà RESULTADOS DEL CASO DE ESTUDIO:\")\n",
    "    print(f\"  üéØ Precisi√≥n predicci√≥n: {metrics_report['summary']['prediction_accuracy']:.1%}\")\n",
    "    print(f\"  ‚è±Ô∏è Reducci√≥n tiempo viaje: {metrics_report['summary']['travel_time_reduction_pct']:.1f}%\") \n",
    "    print(f\"  ‚õΩ Ahorro combustible: {metrics_report['summary']['fuel_savings_pct']:.1f}%\")\n",
    "    print(f\"  üòä Satisfacci√≥n usuario: {metrics_report['summary']['user_satisfaction']:.1f}/100\")\n",
    "    print(f\"  üí∞ ROI: {metrics_report['economic_impact']['roi_years']:.1f} a√±os\")\n",
    "    print(\"\\nüö¶ Sistema listo para optimizaci√≥n de tr√°fico en tiempo real!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74914fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3816e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
