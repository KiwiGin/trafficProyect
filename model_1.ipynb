{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e07478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "layers = keras.layers\n",
    "Lambda = keras.layers.Lambda\n",
    "Model = keras.Model\n",
    "Sequential=keras.models.Sequential\n",
    "MeanSquaredError=keras.losses.MeanSquaredError\n",
    "Precision=keras.metrics.Precision\n",
    "Recall=keras.metrics.Recall\n",
    "MeanAbsoluteError=keras.metrics.MeanAbsoluteError\n",
    "MeanAbsolutePercentageError=keras.metrics.MeanAbsolutePercentageError\n",
    "SparseCategoricalCrossentropy=keras.metrics.SparseCategoricalCrossentropy\n",
    "RootMeanSquaredError=keras.metrics.RootMeanSquaredError\n",
    "Dense=keras.layers.Dense\n",
    "InputLayer=keras.layers.InputLayer\n",
    "LSTM=keras.layers.LSTM\n",
    "ModelCheckpoint=keras.callbacks.ModelCheckpoint\n",
    "Adam=keras.optimizers.Adam\n",
    "load_model=keras.models.load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ea17018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_spark_session():\n",
    "    \"\"\"Configurar Spark para procesar 169M+ registros\"\"\"\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"UTD19_BigData_TrafficPrediction\")\n",
    "        .config(\"spark.driver.memory\", \"4g\")\n",
    "        .config(\"spark.executor.memory\", \"12g\")\n",
    "        .config(\"spark.executor.cores\", \"2\")\n",
    "        .config(\"spark.executor.instances\", \"2\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\")\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(f\"✅ Spark configurado - Versión: {spark.version}\")\n",
    "    print(f\"📊 Configuración: 8 executors, 16GB memoria cada uno\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d7f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UTD19_ETL_Pipeline:\n",
    "    \"\"\"Pipeline ETL completo para dataset UTD19 (134M+ registros)\"\"\"\n",
    "    \n",
    "    def __init__(self, spark_session):\n",
    "        self.spark = spark_session\n",
    "        self.quality_metrics = {}\n",
    "        \n",
    "    def run_complete_etl(self, input_path=\"./data/raw/\", output_path=\"./data/processed/\"):\n",
    "        \"\"\"Ejecutar ETL completo: Extract -> Transform -> Load\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"🚀 INICIANDO ETL PIPELINE COMPLETO UTD19\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # EXTRACT\n",
    "        raw_datasets = self.extract_raw_data(input_path)\n",
    "        \n",
    "        # TRANSFORM\n",
    "        clean_datasets = self.transform_data(raw_datasets)\n",
    "        \n",
    "        # LOAD\n",
    "        self.load_clean_data(clean_datasets, output_path)\n",
    "        \n",
    "        # QUALITY REPORT\n",
    "        self.generate_quality_report()\n",
    "        \n",
    "        return clean_datasets\n",
    "    \n",
    "    def extract_raw_data(self, input_path):\n",
    "        \"\"\"Extracción de datos con validación inicial\"\"\"\n",
    "        print(\"\\n📥 FASE 1: EXTRACCIÓN DE DATOS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Cargar datasets principales\n",
    "        print(\"🔄 Cargando detectores...\")\n",
    "        detectors_df = self.spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"false\") \\\n",
    "            .csv(f\"{input_path}detectors_public.csv\")\n",
    "        \n",
    "        print(\"🔄 Cargando links...\")\n",
    "        links_df = self.spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"false\") \\\n",
    "            .csv(f\"{input_path}links.csv\")\n",
    "        \n",
    "        print(\"🔄 Cargando UTD19 principal (puede tomar varios minutos)...\")\n",
    "        utd19_df = self.spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"false\") \\\n",
    "            .csv(f\"{input_path}utd19_u.csv\") \\\n",
    "            .repartition(400, \"city\", \"detid\")  # Optimizar particionado\n",
    "        \n",
    "        # Estadísticas iniciales\n",
    "        detectors_count = detectors_df.count()\n",
    "        links_count = links_df.count()\n",
    "        utd19_count = utd19_df.count()\n",
    "        \n",
    "        print(f\"✅ Detectores: {detectors_count:,} registros\")\n",
    "        print(f\"✅ Links: {links_count:,} registros\") \n",
    "        print(f\"✅ UTD19: {utd19_count:,} registros\")\n",
    "        \n",
    "        self.quality_metrics['raw_counts'] = {\n",
    "            'detectors': detectors_count,\n",
    "            'links': links_count,\n",
    "            'utd19': utd19_count\n",
    "        }\n",
    "        \n",
    "        return {'detectors': detectors_df, 'links': links_df, 'utd19': utd19_df}\n",
    "    \n",
    "    def transform_data(self, raw_datasets):\n",
    "        \"\"\"Transformaciones de limpieza y enriquecimiento\"\"\"\n",
    "        print(\"\\n🔧 FASE 2: TRANSFORMACIÓN DE DATOS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # 1. Limpieza de tipos de datos\n",
    "        print(\"📋 Paso 1: Enforcement de tipos de datos...\")\n",
    "        clean_datasets = self._enforce_data_types(raw_datasets)\n",
    "        \n",
    "        # 2. Validación de reglas de negocio\n",
    "        print(\"✅ Paso 2: Aplicación de reglas de negocio...\")\n",
    "        validated_datasets = self._apply_business_rules(clean_datasets)\n",
    "        \n",
    "        # 3. Tratamiento de valores faltantes\n",
    "        print(\"🔧 Paso 3: Tratamiento de valores faltantes...\")\n",
    "        imputed_datasets = self._handle_missing_values(validated_datasets)\n",
    "        \n",
    "        # 4. Detección y tratamiento de outliers\n",
    "        print(\"📊 Paso 4: Detección de outliers...\")\n",
    "        final_datasets = self._detect_and_handle_outliers(imputed_datasets)\n",
    "        \n",
    "        # 5. Enriquecimiento con features derivados\n",
    "        print(\"🎯 Paso 5: Creación de features derivados...\")\n",
    "        enriched_datasets = self._create_derived_features(final_datasets)\n",
    "        \n",
    "        return enriched_datasets\n",
    "    \n",
    "    def _enforce_data_types(self, datasets):\n",
    "        \"\"\"Conversión robusta de tipos de datos\"\"\"\n",
    "        print(\"   🔄 Convirtiendo tipos de datos...\")\n",
    "        \n",
    "        # Esquema para UTD19 principal\n",
    "        utd19_schema = {\n",
    "            'detid': 'double',\n",
    "            'city': 'string', \n",
    "            'day': 'string',\n",
    "            'interval': 'double',\n",
    "            'flow': 'double',\n",
    "            'speed': 'double',\n",
    "            'occ': 'double',\n",
    "            'error': 'string'\n",
    "        }\n",
    "        \n",
    "        # Esquema para detectores\n",
    "        detectors_schema = {\n",
    "            'detid': 'double',\n",
    "            'citycode': 'string',\n",
    "            'lat': 'double',\n",
    "            'long': 'double', \n",
    "            'lanes': 'double',\n",
    "            'pos': 'double',\n",
    "            'limit': 'double',\n",
    "            'length': 'double',\n",
    "            'linkid': 'double',\n",
    "            'fclass': 'string',\n",
    "            'road': 'string'\n",
    "        }\n",
    "        \n",
    "        # Aplicar schemas con try_cast para robustez\n",
    "        clean_utd19 = self._apply_schema_robust(datasets['utd19'], utd19_schema)\n",
    "        clean_detectors = self._apply_schema_robust(datasets['detectors'], detectors_schema)\n",
    "        \n",
    "        # Links mantener como está por simplicidad\n",
    "        clean_links = datasets['links']\n",
    "        \n",
    "        return {\n",
    "            'utd19': clean_utd19,\n",
    "            'detectors': clean_detectors, \n",
    "            'links': clean_links\n",
    "        }\n",
    "    \n",
    "    def _apply_schema_robust(self, df, schema):\n",
    "        \"\"\"Aplicar schema con manejo robusto de errores - CORREGIDO\"\"\"\n",
    "        for column, data_type in schema.items():\n",
    "            if column in df.columns:\n",
    "                if data_type == 'int':\n",
    "                    # Para enteros: convertir a double primero, luego a int\n",
    "                    df = df.withColumn(column, \n",
    "                        when(col(column).isNull(), lit(None))\n",
    "                        .when(col(column).rlike(\"^-?[0-9]*\\\\.?[0-9]+$\"), \n",
    "                            col(column).cast(\"double\").cast(\"int\"))\n",
    "                        .otherwise(lit(None))\n",
    "                    )\n",
    "                elif data_type == 'double':\n",
    "                    # Para doubles: usar regex más permisivo\n",
    "                    df = df.withColumn(column, \n",
    "                        when(col(column).isNull(), lit(None))\n",
    "                        .when(col(column).rlike(\"^-?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?$\"),\n",
    "                            col(column).cast(\"double\"))\n",
    "                        .otherwise(lit(None))\n",
    "                    )\n",
    "                else:\n",
    "                    # Para strings, solo limpiar espacios\n",
    "                    df = df.withColumn(column, \n",
    "                        when(col(column).isNull(), lit(None))\n",
    "                        .otherwise(trim(col(column)))\n",
    "                    )\n",
    "        return df\n",
    "    \n",
    "    def _apply_business_rules(self, datasets):\n",
    "        \"\"\"Aplicar reglas de negocio específicas del dominio\"\"\"\n",
    "        print(\"   🎯 Aplicando reglas de negocio de tráfico...\")\n",
    "        \n",
    "        utd19_df = datasets['utd19']\n",
    "        \n",
    "        # Reglas de validación para datos de tráfico\n",
    "        validated_utd19 = utd19_df.filter(\n",
    "            # Flow: 0 a 15,000 vehículos/hora (máximo teórico autopista)\n",
    "            (col(\"flow\").isNull() | col(\"flow\").between(0, 15000)) &\n",
    "            \n",
    "            # Speed: 0 a 200 km/h (máximo razonable urbano/autopista)\n",
    "            (col(\"speed\").isNull() | col(\"speed\").between(0, 200)) &\n",
    "            \n",
    "            # Occupancy: 0 a 1.0 (ratio de ocupación)\n",
    "            (col(\"occ\").isNull() | col(\"occ\").between(0, 1.0)) &\n",
    "            \n",
    "            # Interval: 0 a 86400 segundos (24 horas)\n",
    "            (col(\"interval\").isNull() | col(\"interval\").between(0, 86400)) &\n",
    "            \n",
    "            # Detid debe ser positivo\n",
    "            (col(\"detid\").isNotNull() & (col(\"detid\") > 0))\n",
    "        )\n",
    "        \n",
    "        # Validaciones para detectores\n",
    "        detectors_df = datasets['detectors'] \n",
    "        validated_detectors = detectors_df.filter(\n",
    "            # Coordenadas válidas\n",
    "            (col(\"lat\").isNull() | col(\"lat\").between(-90, 90)) &\n",
    "            (col(\"long\").isNull() | col(\"long\").between(-180, 180)) &\n",
    "            \n",
    "            # Lanes: 1 a 10 carriles (máximo razonable)\n",
    "            (col(\"lanes\").isNull() | col(\"lanes\").between(1, 10)) &\n",
    "            \n",
    "            # Position: 0 a 1 (posición en link)\n",
    "            (col(\"pos\").isNull() | col(\"pos\").between(0, 1)) &\n",
    "            \n",
    "            # Speed limit: 10 a 200 km/h\n",
    "            (col(\"limit\").isNull() | col(\"limit\").between(10, 200))\n",
    "        )\n",
    "        \n",
    "        # Calcular métricas de calidad\n",
    "        original_utd19_count = utd19_df.count()\n",
    "        validated_utd19_count = validated_utd19.count()\n",
    "        \n",
    "        validation_rate = validated_utd19_count / original_utd19_count\n",
    "        \n",
    "        print(f\"   ✅ Registros válidos: {validated_utd19_count:,} de {original_utd19_count:,}\")\n",
    "        print(f\"   📊 Tasa de validación: {validation_rate:.2%}\")\n",
    "        \n",
    "        self.quality_metrics['validation_rate'] = validation_rate\n",
    "        \n",
    "        return {\n",
    "            'utd19': validated_utd19,\n",
    "            'detectors': validated_detectors,\n",
    "            'links': datasets['links']\n",
    "        }\n",
    "    \n",
    "    def _handle_missing_values(self, datasets):\n",
    "        \"\"\"Estrategia de imputación para valores faltantes\"\"\"\n",
    "        print(\"   🔧 Tratando valores faltantes...\")\n",
    "        \n",
    "        utd19_df = datasets['utd19']\n",
    "        \n",
    "        # Estrategias específicas por variable\n",
    "        # 1. Speed: Solo 3.4% completitud - imputar con velocidad límite cuando esté disponible\n",
    "        utd19_df = utd19_df.join(datasets['detectors'].select(\"detid\", col(\"limit\").alias(\"detector_limit\")), \"detid\", \"left\")\n",
    "        \n",
    "        utd19_imputed = utd19_df.withColumn(\n",
    "            \"speed_imputed\",\n",
    "            when(col(\"speed\").isNotNull(), col(\"speed\"))\n",
    "            .when(col(\"detector_limit\").isNotNull(), col(\"detector_limit\") * 0.8)  # 80% del límite\n",
    "            .otherwise(lit(50.0))  # Velocidad urbana promedio\n",
    "        ).withColumn(\n",
    "            \"flow_imputed\", \n",
    "            when(col(\"flow\").isNotNull(), col(\"flow\"))\n",
    "            .otherwise(lit(500))  # Flow promedio urbano\n",
    "        ).withColumn(\n",
    "            \"occ_imputed\",\n",
    "            when(col(\"occ\").isNotNull(), col(\"occ\"))\n",
    "            .otherwise(lit(0.2))  # Ocupancy promedio\n",
    "        )\n",
    "        \n",
    "        # Crear flags de imputación para trazabilidad\n",
    "        utd19_final = utd19_imputed.withColumn(\n",
    "            \"speed_was_imputed\", col(\"speed\").isNull().cast(\"int\")\n",
    "        ).withColumn(\n",
    "            \"flow_was_imputed\", col(\"flow\").isNull().cast(\"int\") \n",
    "        ).withColumn(\n",
    "            \"occ_was_imputed\", col(\"occ\").isNull().cast(\"int\")\n",
    "        ).drop(\"speed\", \"flow\", \"occ\") \\\n",
    "         .withColumnRenamed(\"speed_imputed\", \"speed\") \\\n",
    "         .withColumnRenamed(\"flow_imputed\", \"flow\") \\\n",
    "         .withColumnRenamed(\"occ_imputed\", \"occ\")\n",
    "        \n",
    "        return {\n",
    "            'utd19': utd19_final,\n",
    "            'detectors': datasets['detectors'],\n",
    "            'links': datasets['links']\n",
    "        }\n",
    "    \n",
    "    def _detect_and_handle_outliers(self, datasets):\n",
    "        \"\"\"Detección y tratamiento de outliers estadísticos\"\"\"\n",
    "        print(\"   📊 Detectando outliers...\")\n",
    "        \n",
    "        utd19_df = datasets['utd19']\n",
    "        \n",
    "        # Calcular percentiles para detección de outliers\n",
    "        flow_stats = utd19_df.select(\n",
    "            expr(\"percentile_approx(flow, 0.01)\").alias(\"flow_p1\"),\n",
    "            expr(\"percentile_approx(flow, 0.99)\").alias(\"flow_p99\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        speed_stats = utd19_df.select(\n",
    "            expr(\"percentile_approx(speed, 0.01)\").alias(\"speed_p1\"), \n",
    "            expr(\"percentile_approx(speed, 0.99)\").alias(\"speed_p99\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        # Aplicar winsorización (cap outliers a percentiles)\n",
    "        utd19_clean = utd19_df.withColumn(\n",
    "            \"flow_clean\",\n",
    "            when(col(\"flow\") < flow_stats['flow_p1'], lit(flow_stats['flow_p1']))\n",
    "            .when(col(\"flow\") > flow_stats['flow_p99'], lit(flow_stats['flow_p99']))\n",
    "            .otherwise(col(\"flow\"))\n",
    "        ).withColumn(\n",
    "            \"speed_clean\", \n",
    "            when(col(\"speed\") < speed_stats['speed_p1'], lit(speed_stats['speed_p1']))\n",
    "            .when(col(\"speed\") > speed_stats['speed_p99'], lit(speed_stats['speed_p99']))\n",
    "            .otherwise(col(\"speed\"))\n",
    "        ).drop(\"flow\", \"speed\") \\\n",
    "         .withColumnRenamed(\"flow_clean\", \"flow\") \\\n",
    "         .withColumnRenamed(\"speed_clean\", \"speed\")\n",
    "        \n",
    "        return {\n",
    "            'utd19': utd19_clean,\n",
    "            'detectors': datasets['detectors'],\n",
    "            'links': datasets['links']\n",
    "        }\n",
    "    \n",
    "    def _create_derived_features(self, datasets):\n",
    "        \"\"\"Crear features derivados para ML\"\"\"\n",
    "        print(\"   🎯 Creando features derivados...\")\n",
    "        \n",
    "        utd19_df = datasets['utd19']\n",
    "        detectors_df = datasets['detectors']\n",
    "        \n",
    "        # Join principal\n",
    "        enhanced_df = utd19_df.join(detectors_df, \"detid\", \"left\")\n",
    "        \n",
    "        # Features temporales\n",
    "        enhanced_df = enhanced_df.withColumn(\n",
    "            \"hour_of_day\", (col(\"interval\") / 3600).cast(\"int\")\n",
    "        ).withColumn(\n",
    "            \"day_of_week\", dayofweek(to_date(col(\"day\"), \"yyyy-MM-dd\"))\n",
    "        ).withColumn(\n",
    "            \"is_weekend\", when(col(\"day_of_week\").isin([1, 7]), 1).otherwise(0)\n",
    "        ).withColumn(\n",
    "            \"is_rush_hour\",\n",
    "            when(col(\"hour_of_day\").between(7, 9) | col(\"hour_of_day\").between(17, 19), 1).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        # Features de tráfico\n",
    "        enhanced_df = enhanced_df.withColumn(\n",
    "            \"speed_ratio\",\n",
    "            when(col(\"limit\") > 0, least(col(\"speed\") / col(\"limit\"), lit(2.0))).otherwise(1.0)\n",
    "        ).withColumn(\n",
    "            \"congestion_level\",\n",
    "            when(col(\"speed_ratio\") >= 0.8, 0)\n",
    "            .when(col(\"speed_ratio\") >= 0.6, 1) \n",
    "            .when(col(\"speed_ratio\") >= 0.4, 2)\n",
    "            .when(col(\"speed_ratio\") >= 0.2, 3)\n",
    "            .otherwise(4)\n",
    "        )\n",
    "        \n",
    "        # Features espaciales\n",
    "        enhanced_df = enhanced_df.withColumn(\n",
    "            \"near_intersection\", when(col(\"pos\") <= 0.1, 1).otherwise(0)\n",
    "        ).withColumn(\n",
    "            \"multi_lane\", when(col(\"lanes\") > 1, 1).otherwise(0)\n",
    "        ).withColumn(\n",
    "            \"has_speed_limit\", when(col(\"limit\") > 0, 1).otherwise(0)\n",
    "        )\n",
    "\n",
    "        # features temporales derivadas\n",
    "        w = Window.partitionBy(\"detid\").orderBy(\"day\",\"interval\")\n",
    "        enhanced_df = enhanced_df.withColumn(\"flow_lag_1\", lag(\"flow\", 1).over(w))\n",
    "        enhanced_df = enhanced_df.withColumn(\"speed_trend\", col(\"speed\") - lag(\"speed\", 1).over(w))\n",
    "        \n",
    "        # Cache para operaciones posteriores\n",
    "        enhanced_df.cache()\n",
    "        \n",
    "        return {\n",
    "            'utd19_ml_ready': enhanced_df,\n",
    "            'detectors': datasets['detectors'],\n",
    "            'links': datasets['links']\n",
    "        }\n",
    "    \n",
    "    def load_clean_data(self, clean_datasets, output_path):\n",
    "        \"\"\"Persistir datos limpios - VERSION WINDOWS COMPATIBLE\"\"\"\n",
    "        print(f\"\\n💾 FASE 3: CARGA DE DATOS LIMPIOS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Guardar en CSV para evitar problemas de Hadoop en Windows\n",
    "        print(\"🔄 Guardando dataset principal...\")\n",
    "        clean_datasets['utd19_ml_ready'].coalesce(10).write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .csv(f\"{output_path}utd19clean\")\n",
    "\n",
    "        print(\"🔄 Guardando detectores...\")\n",
    "        clean_datasets['detectors'].write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .csv(f\"{output_path}detectorsclean\")\n",
    "\n",
    "        print(\"🔄 Guardando links...\")\n",
    "        clean_datasets['links'].write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .csv(f\"{output_path}linksclean\")\n",
    "        \n",
    "        print(f\"✅ Datos guardados en: {output_path}\")\n",
    "    \n",
    "    def generate_quality_report(self):\n",
    "        \"\"\"Generar reporte de calidad de datos\"\"\"\n",
    "        print(f\"\\n📋 REPORTE DE CALIDAD DE DATOS\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        print(f\"📊 Registros originales: {self.quality_metrics['raw_counts']['utd19']:,}\")\n",
    "        print(f\"✅ Tasa de validación: {self.quality_metrics['validation_rate']:.2%}\")\n",
    "        \n",
    "        # Calcular registros finales estimados\n",
    "        final_estimated = int(self.quality_metrics['raw_counts']['utd19'] * self.quality_metrics['validation_rate'])\n",
    "        print(f\"🎯 Registros limpios estimados: {final_estimated:,}\")\n",
    "        \n",
    "        reduction_rate = 1 - self.quality_metrics['validation_rate']\n",
    "        print(f\"📉 Reducción por limpieza: {reduction_rate:.2%}\")\n",
    "        \n",
    "        if self.quality_metrics['validation_rate'] > 0.8:\n",
    "            print(\"✅ CALIDAD: Excelente (>80% datos válidos)\")\n",
    "        elif self.quality_metrics['validation_rate'] > 0.6:\n",
    "            print(\"⚠️ CALIDAD: Buena (>60% datos válidos)\")\n",
    "        else:\n",
    "            print(\"🔴 CALIDAD: Requiere atención (<60% datos válidos)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ed5c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatioTemporalLSTM(tf.keras.Model):\n",
    "    def __init__(self, num_detectors, sequence_length=12, feature_dim=10):\n",
    "        super().__init__()\n",
    "        self.num_detectors = num_detectors\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Embedding espacial para detectores\n",
    "        self.detector_embedding = layers.Embedding(\n",
    "            input_dim=num_detectors + 1,  # +1 para detectores desconocidos\n",
    "            output_dim=32,\n",
    "            mask_zero=True,\n",
    "            name=\"detector_embedding\"\n",
    "        )\n",
    "        \n",
    "        # Capas LSTM para modelado temporal\n",
    "        self.lstm1 = layers.LSTM(\n",
    "            128, \n",
    "            return_sequences=True, \n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.0,\n",
    "            name=\"lstm_1\"\n",
    "        )\n",
    "        self.lstm2 = layers.LSTM(\n",
    "            64, \n",
    "            return_sequences=False,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.0,\n",
    "            name=\"lstm_2\"\n",
    "        )\n",
    "        \n",
    "        # Capas densas para procesamiento\n",
    "        self.dense1 = Dense(64, activation='relu', name=\"dense_1\")\n",
    "        self.dropout1 = layers.Dropout(0.3)\n",
    "        self.dense2 = Dense(32, activation='relu', name=\"dense_2\")\n",
    "        self.dropout2 = layers.Dropout(0.2)\n",
    "        \n",
    "        # Salidas múltiples\n",
    "        self.flow_output = Dense(1, activation='linear', name='flow_prediction')\n",
    "        self.congestion_output = Dense(2, activation='softmax', name='congestion_prediction')\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs: [temporal_features, detector_ids]\n",
    "        temporal_features, detector_ids = inputs\n",
    "        \n",
    "        # Procesar embeddings espaciales\n",
    "        detector_emb = self.detector_embedding(detector_ids)\n",
    "        \n",
    "        # Combinar features temporales con embeddings espaciales\n",
    "        # temporal_features: [batch, sequence, features]\n",
    "        # detector_emb: [batch, embedding_dim]\n",
    "        detector_emb_expanded = tf.expand_dims(detector_emb, axis=1)\n",
    "        detector_emb_tiled = tf.tile(detector_emb_expanded, [1, self.sequence_length, 1])\n",
    "        \n",
    "        combined_features = tf.concat([temporal_features, detector_emb_tiled], axis=-1)\n",
    "        \n",
    "        # Procesamiento LSTM\n",
    "        lstm_out = self.lstm1(combined_features, training=training)\n",
    "        lstm_out = self.lstm2(lstm_out, training=training)\n",
    "        \n",
    "        # Capas densas\n",
    "        dense_out = self.dense1(lstm_out)\n",
    "        dense_out = self.dropout1(dense_out, training=training)\n",
    "        dense_out = self.dense2(dense_out)\n",
    "        dense_out = self.dropout2(dense_out, training=training)\n",
    "        \n",
    "        # Predicciones\n",
    "        flow_pred = self.flow_output(dense_out)\n",
    "        congestion_pred = self.congestion_output(dense_out)\n",
    "        \n",
    "        return {\n",
    "            'flow_prediction': flow_pred,\n",
    "            'congestion_prediction': congestion_pred\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1af0fb9d-d9f6-496b-8b01-6bec32d2481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_traffic_prediction_data(ml_df_pandas, sequence_length=12, test_weeks=1):\n",
    "    \"\"\"\n",
    "    División temporal específica para predicción de tráfico\n",
    "    \n",
    "    Args:\n",
    "        ml_df_pandas: DataFrame con datos de tráfico\n",
    "        sequence_length: Ventana temporal (12 intervalos = 1 hora si cada intervalo es 5min)\n",
    "        test_weeks: Número de semanas para testing (simula predicción real)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🚦 PREPARANDO DATOS PARA PREDICCIÓN DE TRÁFICO\")\n",
    "    print(f\"📊 Total de registros: {len(ml_df_pandas):,}\")\n",
    "    \n",
    "    # ===== PASO 1: NORMALIZACIÓN (UNA SOLA VEZ) =====\n",
    "    temporal_features = [\n",
    "        'flow', 'speed', 'occ', 'hour_of_day', 'is_rush_hour',\n",
    "        'flow_lag_1', 'speed_trend', 'lanes', 'pos', 'near_intersection',\n",
    "        'length', 'has_speed_limit'\n",
    "    ]\n",
    "    \n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    features_to_scale = temporal_features\n",
    "\n",
    "    ml_df_pandas[features_to_scale] = ml_df_pandas[features_to_scale].fillna(0)\n",
    "    \n",
    "    # Normalizar (excepto flow que es nuestro target principal)\n",
    "    ml_df_pandas[features_to_scale] = scaler.fit_transform(ml_df_pandas[features_to_scale])\n",
    "    \n",
    "    # ===== PASO 2: DIVISIÓN TEMPORAL REALISTA =====\n",
    "    # Ordenar por tiempo (CRÍTICO para series temporales)\n",
    "    ml_df_pandas = ml_df_pandas.sort_values(['day', 'interval']).reset_index(drop=True)\n",
    "    \n",
    "    # Calcular días de test (últimas semanas)\n",
    "    unique_days = sorted(ml_df_pandas['day'].unique())\n",
    "    test_days_count = test_weeks * 7  # días\n",
    "    \n",
    "    if test_days_count >= len(unique_days):\n",
    "        test_days_count = max(1, len(unique_days) // 4)  # Mínimo 25% para test\n",
    "    \n",
    "    train_days = unique_days[:-test_days_count]\n",
    "    test_days = unique_days[-test_days_count:]\n",
    "    \n",
    "    print(f\"📅 Período de entrenamiento: días {train_days[0]} a {train_days[-1]} ({len(train_days)} días)\")\n",
    "    print(f\"📅 Período de prueba: días {test_days[0]} a {test_days[-1]} ({len(test_days)} días)\")\n",
    "    \n",
    "    # ===== PASO 3: CREAR SECUENCIAS POR DETECTOR =====\n",
    "    unique_detectors = ml_df_pandas['detid'].unique()\n",
    "    detector_to_id = {det: i+1 for i, det in enumerate(unique_detectors)}\n",
    "    \n",
    "    # Contenedores para train y test\n",
    "    train_sequences = []\n",
    "    test_sequences = []\n",
    "    \n",
    "    print(f\"🚗 Procesando {len(unique_detectors)} detectores...\")\n",
    "    \n",
    "    for detector in unique_detectors:\n",
    "        detector_data = ml_df_pandas[ml_df_pandas['detid'] == detector].sort_values(['day', 'interval'])\n",
    "        \n",
    "        if len(detector_data) < sequence_length:\n",
    "            continue\n",
    "        \n",
    "        # Crear secuencias para este detector\n",
    "        for i in range(sequence_length, len(detector_data)):\n",
    "            # Secuencia de entrada (12 intervalos anteriores)\n",
    "            sequence = detector_data.iloc[i-sequence_length:i][temporal_features].values\n",
    "            \n",
    "            # Target (intervalo actual)\n",
    "            target_row = detector_data.iloc[i]\n",
    "            target_day = target_row['day']\n",
    "            \n",
    "            sample = {\n",
    "                'X_temporal': sequence,\n",
    "                'X_detector': detector_to_id[detector],\n",
    "                'y_flow': target_row['flow'],\n",
    "                'y_congestion': target_row['congestion_level'],\n",
    "                'day': target_day,\n",
    "                'detector': detector,\n",
    "                'interval': target_row['interval']\n",
    "            }\n",
    "            \n",
    "            # Asignar a train o test basándose en el día\n",
    "            if target_day in test_days:\n",
    "                test_sequences.append(sample)\n",
    "            else:\n",
    "                train_sequences.append(sample)\n",
    "    \n",
    "    print(f\"✅ Secuencias de entrenamiento: {len(train_sequences):,}\")\n",
    "    print(f\"✅ Secuencias de prueba: {len(test_sequences):,}\")\n",
    "    \n",
    "    # ===== PASO 4: CONVERTIR A ARRAYS =====\n",
    "    def sequences_to_arrays(sequences):\n",
    "        if not sequences:\n",
    "            return tuple([np.array([]) for _ in range(4)])\n",
    "        \n",
    "        X_temporal = np.array([s['X_temporal'] for s in sequences])\n",
    "        X_detector = np.array([s['X_detector'] for s in sequences])\n",
    "        y_flow = np.array([s['y_flow'] for s in sequences])\n",
    "        y_congestion = np.array([s['y_congestion'] for s in sequences])\n",
    "        \n",
    "        return X_temporal, X_detector, y_flow, y_congestion\n",
    "    \n",
    "    X_temp_train, X_det_train, y_flow_train, y_cong_train = sequences_to_arrays(train_sequences)\n",
    "    X_temp_test, X_det_test, y_flow_test, y_cong_test = sequences_to_arrays(test_sequences)\n",
    "    \n",
    "    # ===== PASO 5: VALIDACIÓN DE DATOS =====\n",
    "    print(f\"\\n📊 RESUMEN DE DATOS:\")\n",
    "    print(f\"Train - Temporal: {X_temp_train.shape}, Flow: {y_flow_train.shape}\")\n",
    "    print(f\"Test  - Temporal: {X_temp_test.shape}, Flow: {y_flow_test.shape}\")\n",
    "    \n",
    "    # Verificar distribución de congestión\n",
    "    train_cong_dist = np.bincount(y_cong_train.astype(int))\n",
    "    test_cong_dist = np.bincount(y_cong_test.astype(int))\n",
    "    \n",
    "    print(f\"\\n🚦 Distribución de congestión:\")\n",
    "    print(f\"Train: {dict(enumerate(train_cong_dist))}\")\n",
    "    print(f\"Test:  {dict(enumerate(test_cong_dist))}\")\n",
    "    \n",
    "    # Verificar rangos de flow\n",
    "    print(f\"\\n🌊 Estadísticas de Flow:\")\n",
    "    print(f\"Train - Min: {y_flow_train.min():.1f}, Max: {y_flow_train.max():.1f}, Mean: {y_flow_train.mean():.1f}\")\n",
    "    print(f\"Test  - Min: {y_flow_test.min():.1f}, Max: {y_flow_test.max():.1f}, Mean: {y_flow_test.mean():.1f}\")\n",
    "    \n",
    "    return {\n",
    "        'train': (X_temp_train, X_det_train, y_flow_train, y_cong_train),\n",
    "        'test': (X_temp_test, X_det_test, y_flow_test, y_cong_test),\n",
    "        'metadata': {\n",
    "            'scaler': scaler,\n",
    "            'detector_mapping': detector_to_id,\n",
    "            'train_days': train_days,\n",
    "            'test_days': test_days,\n",
    "            'sequence_length': sequence_length,\n",
    "            'temporal_features': temporal_features\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d756631d-b56c-4994-8327-388b4d6a50e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_traffic_data(ml_df_pandas):\n",
    "    \"\"\"Función simplificada para usar en tu pipeline\"\"\"\n",
    "    \n",
    "    results = prepare_traffic_prediction_data(ml_df_pandas, sequence_length=12, test_weeks=1)\n",
    "    \n",
    "    # Extraer datos\n",
    "    X_temp_train, X_det_train, y_flow_train, y_cong_train = results['train']\n",
    "    X_temp_test, X_det_test, y_flow_test, y_cong_test = results['test']\n",
    "    \n",
    "    # Extraer metadata\n",
    "    scaler = results['metadata']['scaler']\n",
    "    detector_mapping = results['metadata']['detector_mapping']\n",
    "\n",
    "    # --- 🔹 Escalar target flow ---\n",
    "    flow_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    # reshape porque MinMaxScaler espera 2D\n",
    "    y_flow_train_scaled = flow_scaler.fit_transform(y_flow_train.reshape(-1, 1))\n",
    "    y_flow_test_scaled  = flow_scaler.transform(y_flow_test.reshape(-1, 1))\n",
    "\n",
    "    # Etiquetas de congestión a int32\n",
    "    y_cong_train = y_cong_train.astype(np.int32)\n",
    "    y_cong_test  = y_cong_test.astype(np.int32)\n",
    "    \n",
    "    # Devuelve todo, incluido el scaler de flow\n",
    "    return (\n",
    "        X_temp_train, X_temp_test,\n",
    "        X_det_train, X_det_test,\n",
    "        y_flow_train_scaled, y_flow_test_scaled,   # << usar los escalados\n",
    "        y_cong_train, y_cong_test,\n",
    "        scaler, detector_mapping, flow_scaler      # << también devuelves flow_scaler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "662bf8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficManagementMetrics:\n",
    "    def __init__(self):\n",
    "        self.baseline_metrics = {}\n",
    "        self.optimized_metrics = {}\n",
    "    \n",
    "    def calculate_prediction_accuracy(self, model, X_test, y_true_flow, y_true_congestion):\n",
    "        \"\"\"Métrica 1: Prediction accuracy\"\"\"\n",
    "        predictions = model.predict([X_test[0], X_test[1]])\n",
    "        \n",
    "        # Accuracy para predicción de congestión\n",
    "        from sklearn.metrics import accuracy_score, classification_report\n",
    "        y_pred_congestion = np.argmax(predictions['congestion_prediction'], axis=1)\n",
    "        congestion_accuracy = accuracy_score(y_true_congestion, y_pred_congestion)\n",
    "        \n",
    "        # MAE para predicción de flujo\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        flow_mae = mean_absolute_error(y_true_flow, predictions['flow_prediction'])\n",
    "        \n",
    "        print(f\"📊 MÉTRICA 1 - PREDICTION ACCURACY:\")\n",
    "        print(f\"  🚦 Congestión Accuracy: {congestion_accuracy:.3f}\")\n",
    "        print(f\"  🚗 Flujo MAE: {flow_mae:.2f} veh/h\")\n",
    "        print(f\"  📈 Score General: {(congestion_accuracy * 100):.1f}%\")\n",
    "\n",
    "        # Agregar al final del método calculate_prediction_accuracy:\n",
    "        \n",
    "        # Gráfico de accuracy por nivel de congestión\n",
    "        from sklearn.metrics import classification_report\n",
    "        # === NUEVO BLOQUE para classification_report dinámico ===\n",
    "        # Determinar clases presentes\n",
    "        labels = np.unique(np.concatenate([y_true_congestion, y_pred_congestion]))\n",
    "        all_classes = ['Free', 'Light', 'Moderate', 'Heavy', 'Severe']\n",
    "        target_names_present = [all_classes[i] for i in labels]\n",
    "    \n",
    "        print(\"\\n📊 REPORTE DE CLASIFICACIÓN:\")\n",
    "        report = classification_report(\n",
    "            y_true_congestion,\n",
    "            y_pred_congestion,\n",
    "            labels=labels,\n",
    "            target_names=target_names_present,\n",
    "            output_dict=True\n",
    "        )\n",
    "    \n",
    "        # Visualizar F1-score por cada clase presente\n",
    "        f1_scores = [report[cls]['f1-score'] for cls in target_names_present]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(classes, f1_scores, color=['green', 'yellow', 'orange', 'red', 'darkred'])\n",
    "        plt.title('F1-Score por Nivel de Congestión')\n",
    "        plt.xlabel('Nivel de Congestión')\n",
    "        plt.ylabel('F1-Score')\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # Agregar valores en las barras\n",
    "        for bar, score in zip(bars, f1_scores):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{score:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig('f1_scores_by_congestion_level.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'congestion_accuracy': congestion_accuracy,\n",
    "            'flow_mae': flow_mae,\n",
    "            'overall_score': congestion_accuracy\n",
    "        }\n",
    "    \n",
    "    def calculate_travel_time_reduction(self, baseline_data, optimized_data):\n",
    "        \"\"\"Métrica 2: Reduction in average travel time\"\"\"\n",
    "        # Calcular tiempo promedio de viaje basado en velocidad y distancia\n",
    "        def calculate_avg_travel_time(data):\n",
    "            # Usar velocidad promedio para estimar tiempo de viaje\n",
    "            avg_speed = data['speed'].mean()\n",
    "            # Asumir distancia promedio de 5 km para viajes urbanos\n",
    "            avg_distance_km = 5.0\n",
    "            avg_travel_time_hours = avg_distance_km / avg_speed if avg_speed > 0 else 1.0\n",
    "            return avg_travel_time_hours * 60  # Convertir a minutos\n",
    "        \n",
    "        baseline_time = calculate_avg_travel_time(baseline_data)\n",
    "        optimized_time = calculate_avg_travel_time(optimized_data)\n",
    "        \n",
    "        time_reduction = baseline_time - optimized_time\n",
    "        reduction_percentage = (time_reduction / baseline_time) * 100\n",
    "        \n",
    "        print(f\"📊 MÉTRICA 2 - TRAVEL TIME REDUCTION:\")\n",
    "        print(f\"  ⏱️ Tiempo baseline: {baseline_time:.1f} min\")\n",
    "        print(f\"  ⚡ Tiempo optimizado: {optimized_time:.1f} min\")\n",
    "        print(f\"  📉 Reducción: {time_reduction:.1f} min ({reduction_percentage:.1f}%)\")\n",
    "        \n",
    "        return {\n",
    "            'baseline_time_min': baseline_time,\n",
    "            'optimized_time_min': optimized_time,\n",
    "            'reduction_min': time_reduction,\n",
    "            'reduction_percentage': reduction_percentage\n",
    "        }\n",
    "    \n",
    "    def calculate_fuel_savings(self, baseline_data, optimized_data):\n",
    "        \"\"\"Métrica 3: Fuel savings\"\"\"\n",
    "        # Fórmula simplificada: consumo inversamente proporcional a velocidad\n",
    "        def calculate_fuel_consumption(data):\n",
    "            # Consumo base: 8L/100km a 50 km/h\n",
    "            base_consumption = 8.0  # L/100km\n",
    "            reference_speed = 50.0  # km/h\n",
    "            \n",
    "            # Factor de corrección por velocidad (más lento = más consumo)\n",
    "            speed_factor = data['speed'].apply(\n",
    "                lambda s: max(1.0, reference_speed / max(s, 10))  # Evitar división por 0\n",
    "            )\n",
    "            \n",
    "            # Consumo por km\n",
    "            consumption_per_km = (base_consumption / 100) * speed_factor\n",
    "            \n",
    "            # Asumir 5 km promedio de distancia\n",
    "            total_consumption = consumption_per_km * 5.0\n",
    "            \n",
    "            return total_consumption.mean()\n",
    "        \n",
    "        baseline_fuel = calculate_fuel_consumption(baseline_data)\n",
    "        optimized_fuel = calculate_fuel_consumption(optimized_data)\n",
    "        \n",
    "        fuel_savings = baseline_fuel - optimized_fuel\n",
    "        savings_percentage = (fuel_savings / baseline_fuel) * 100\n",
    "        \n",
    "        # Convertir a ahorro monetario (precio combustible ~$1.50/L)\n",
    "        fuel_price_per_liter = 1.50\n",
    "        monetary_savings = fuel_savings * fuel_price_per_liter\n",
    "        \n",
    "        print(f\"📊 MÉTRICA 3 - FUEL SAVINGS:\")\n",
    "        print(f\"  ⛽ Consumo baseline: {baseline_fuel:.2f} L/viaje\")\n",
    "        print(f\"  🌱 Consumo optimizado: {optimized_fuel:.2f} L/viaje\")\n",
    "        print(f\"  💰 Ahorro: {fuel_savings:.2f} L/viaje ({savings_percentage:.1f}%)\")\n",
    "        print(f\"  💵 Ahorro monetario: ${monetary_savings:.2f}/viaje\")\n",
    "        \n",
    "        return {\n",
    "            'baseline_fuel_L': baseline_fuel,\n",
    "            'optimized_fuel_L': optimized_fuel,\n",
    "            'savings_L': fuel_savings,\n",
    "            'savings_percentage': savings_percentage,\n",
    "            'monetary_savings_USD': monetary_savings\n",
    "        }\n",
    "    \n",
    "    def simulate_user_satisfaction(self, travel_time_reduction, fuel_savings):\n",
    "        \"\"\"Métrica 4: User satisfaction (simulada)\"\"\"\n",
    "        # Simulación basada en mejoras objetivas\n",
    "        time_satisfaction = min(100, max(0, 50 + (travel_time_reduction['reduction_percentage'] * 2)))\n",
    "        fuel_satisfaction = min(100, max(0, 50 + (fuel_savings['savings_percentage'] * 3)))\n",
    "        \n",
    "        # Factores adicionales simulados\n",
    "        reliability_score = 85  # Consistencia del sistema\n",
    "        ease_of_use_score = 78  # Facilidad de uso\n",
    "        \n",
    "        overall_satisfaction = (time_satisfaction + fuel_satisfaction + \n",
    "                              reliability_score + ease_of_use_score) / 4\n",
    "        \n",
    "        print(f\"📊 MÉTRICA 4 - USER SATISFACTION:\")\n",
    "        print(f\"  ⏱️ Satisfacción tiempo: {time_satisfaction:.1f}/100\")\n",
    "        print(f\"  ⛽ Satisfacción combustible: {fuel_satisfaction:.1f}/100\")\n",
    "        print(f\"  🔄 Confiabilidad: {reliability_score}/100\")\n",
    "        print(f\"  📱 Facilidad uso: {ease_of_use_score}/100\")\n",
    "        print(f\"  🎯 Satisfacción general: {overall_satisfaction:.1f}/100\")\n",
    "        \n",
    "        return {\n",
    "            'time_satisfaction': time_satisfaction,\n",
    "            'fuel_satisfaction': fuel_satisfaction,\n",
    "            'reliability_score': reliability_score,\n",
    "            'ease_of_use_score': ease_of_use_score,\n",
    "            'overall_satisfaction': overall_satisfaction\n",
    "        }\n",
    "    \n",
    "    def generate_final_report(self, prediction_metrics, travel_time_metrics, \n",
    "                            fuel_metrics, satisfaction_metrics):\n",
    "        \"\"\"Generar reporte final con todas las métricas del caso de estudio\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📋 REPORTE FINAL - CASE STUDY METRICS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\n🎯 RESUMEN EJECUTIVO:\")\n",
    "        print(f\"  ✅ Precisión de predicción: {prediction_metrics['overall_score']:.1%}\")\n",
    "        print(f\"  ⏱️ Reducción tiempo viaje: {travel_time_metrics['reduction_percentage']:.1f}%\")\n",
    "        print(f\"  ⛽ Ahorro combustible: {fuel_metrics['savings_percentage']:.1f}%\")\n",
    "        print(f\"  😊 Satisfacción usuario: {satisfaction_metrics['overall_satisfaction']:.1f}/100\")\n",
    "        \n",
    "        # Impacto económico\n",
    "        daily_trips = 10000  # Estimación para una ciudad\n",
    "        daily_savings = fuel_metrics['monetary_savings_USD'] * daily_trips\n",
    "        annual_savings = daily_savings * 365\n",
    "        \n",
    "        print(f\"\\n💰 IMPACTO ECONÓMICO ESTIMADO:\")\n",
    "        print(f\"  📊 Viajes diarios estimados: {daily_trips:,}\")\n",
    "        print(f\"  💵 Ahorro diario: ${daily_savings:,.2f}\")\n",
    "        print(f\"  🏦 Ahorro anual: ${annual_savings:,.2f}\")\n",
    "        \n",
    "        # ROI del sistema\n",
    "        system_cost = 5000000  # $5M estimado\n",
    "        roi_years = system_cost / annual_savings\n",
    "        \n",
    "        print(f\"  📈 ROI del sistema: {roi_years:.1f} años\")\n",
    "        \n",
    "        return {\n",
    "            'summary': {\n",
    "                'prediction_accuracy': prediction_metrics['overall_score'],\n",
    "                'travel_time_reduction_pct': travel_time_metrics['reduction_percentage'],\n",
    "                'fuel_savings_pct': fuel_metrics['savings_percentage'],\n",
    "                'user_satisfaction': satisfaction_metrics['overall_satisfaction']\n",
    "            },\n",
    "            'economic_impact': {\n",
    "                'daily_savings_USD': daily_savings,\n",
    "                'annual_savings_USD': annual_savings,\n",
    "                'roi_years': roi_years\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e8b658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== VISUALIZACIÓN DE RESULTADOS =====\n",
    "class TrafficVisualization:\n",
    "    def __init__(self):\n",
    "        plt.style.use('default')\n",
    "        sns.set_palette(\"husl\")\n",
    "    \n",
    "    def plot_data_distribution(self, ml_df_pandas):\n",
    "        \"\"\"Gráficos de distribución de datos\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('UTD19 Dataset - Distribución de Variables', fontsize=16)\n",
    "        \n",
    "        # Distribución de flujo\n",
    "        axes[0,0].hist(ml_df_pandas['flow'].dropna(), bins=50, alpha=0.7)\n",
    "        axes[0,0].set_title('Distribución de Flujo (veh/h)')\n",
    "        axes[0,0].set_xlabel('Flujo')\n",
    "        axes[0,0].set_ylabel('Frecuencia')\n",
    "        \n",
    "        # Distribución de velocidad\n",
    "        axes[0,1].hist(ml_df_pandas['speed'].dropna(), bins=50, alpha=0.7, color='orange')\n",
    "        axes[0,1].set_title('Distribución de Velocidad (km/h)')\n",
    "        axes[0,1].set_xlabel('Velocidad')\n",
    "        \n",
    "        # Distribución de ocupancy\n",
    "        axes[0,2].hist(ml_df_pandas['occ'].dropna(), bins=50, alpha=0.7, color='green')\n",
    "        axes[0,2].set_title('Distribución de Ocupancy')\n",
    "        axes[0,2].set_xlabel('Ocupancy')\n",
    "        \n",
    "        # Congestión por hora del día\n",
    "        congestion_hour = ml_df_pandas.groupby('hour_of_day')['congestion_level'].mean()\n",
    "        axes[1,0].plot(congestion_hour.index, congestion_hour.values, marker='o')\n",
    "        axes[1,0].set_title('Congestión Promedio por Hora')\n",
    "        axes[1,0].set_xlabel('Hora del día')\n",
    "        axes[1,0].set_ylabel('Nivel de congestión')\n",
    "        \n",
    "        # Distribución por ciudad\n",
    "        city_counts = ml_df_pandas['city'].value_counts().head(10)\n",
    "        axes[1,1].bar(range(len(city_counts)), city_counts.values)\n",
    "        axes[1,1].set_title('Top 10 Ciudades por Registros')\n",
    "        axes[1,1].set_xticks(range(len(city_counts)))\n",
    "        axes[1,1].set_xticklabels(city_counts.index, rotation=45)\n",
    "        \n",
    "        # Heatmap de correlación\n",
    "        corr_vars = ['flow', 'speed', 'occ', 'hour_of_day', 'congestion_level']\n",
    "        corr_matrix = ml_df_pandas[corr_vars].corr()\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1,2])\n",
    "        axes[1,2].set_title('Matriz de Correlación')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('utd19_data_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_training_history(self, history):\n",
    "        \"\"\"Gráfico de entrenamiento del modelo\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Historial de Entrenamiento del Modelo', fontsize=16)\n",
    "        \n",
    "        # Loss total\n",
    "        axes[0,0].plot(history.history['loss'], label='Train Loss')\n",
    "        axes[0,0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "        axes[0,0].set_title('Loss Total')\n",
    "        axes[0,0].set_xlabel('Época')\n",
    "        axes[0,0].set_ylabel('Loss')\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].grid(True)\n",
    "        \n",
    "        # Flow prediction MAE\n",
    "        axes[0,1].plot(history.history['flow_prediction_mean_absolute_error'], label='Train MAE')\n",
    "        axes[0,1].plot(history.history['val_flow_prediction_mean_absolute_error'], label='Val MAE')\n",
    "        axes[0,1].set_title('Flow Prediction - MAE')\n",
    "        axes[0,1].set_xlabel('Época')\n",
    "        axes[0,1].set_ylabel('MAE')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True)\n",
    "        \n",
    "        # Congestion accuracy\n",
    "        axes[1,0].plot(history.history['congestion_prediction_accuracy'], label='Train Acc')\n",
    "        axes[1,0].plot(history.history['val_congestion_prediction_accuracy'], label='Val Acc')\n",
    "        axes[1,0].set_title('Congestion Prediction - Accuracy')\n",
    "        axes[1,0].set_xlabel('Época')\n",
    "        axes[1,0].set_ylabel('Accuracy')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True)\n",
    "        \n",
    "        # Learning curves combined\n",
    "        axes[1,1].plot(history.history['loss'], label='Train Loss', alpha=0.7)\n",
    "        axes[1,1].plot(history.history['val_loss'], label='Val Loss', alpha=0.7)\n",
    "        ax2 = axes[1,1].twinx()\n",
    "        ax2.plot(history.history['congestion_prediction_accuracy'], \n",
    "                label='Train Acc', color='green', alpha=0.7)\n",
    "        ax2.plot(history.history['val_congestion_prediction_accuracy'], \n",
    "                label='Val Acc', color='red', alpha=0.7)\n",
    "        axes[1,1].set_title('Loss vs Accuracy')\n",
    "        axes[1,1].set_xlabel('Época')\n",
    "        axes[1,1].set_ylabel('Loss')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrix(self, y_true, y_pred):\n",
    "        \"\"\"Matriz de confusión para clasificación de congestión\"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=['Free', 'Light', 'Moderate', 'Heavy', 'Severe'],\n",
    "                   yticklabels=['Free', 'Light', 'Moderate', 'Heavy', 'Severe'])\n",
    "        plt.title('Matriz de Confusión - Predicción de Congestión', fontsize=14)\n",
    "        plt.xlabel('Predicción')\n",
    "        plt.ylabel('Valor Real')\n",
    "        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Classification report\n",
    "        print(\"\\n📊 REPORTE DE CLASIFICACIÓN:\")\n",
    "        print(classification_report(y_true, y_pred, \n",
    "                                  target_names=['Free', 'Light', 'Moderate', 'Heavy', 'Severe']))\n",
    "    \n",
    "    def plot_predictions_vs_actual(self, y_true_flow, y_pred_flow, sample_size=1000):\n",
    "        \"\"\"Scatter plot de predicciones vs valores reales\"\"\"\n",
    "        # Tomar muestra para visualización\n",
    "        indices = np.random.choice(len(y_true_flow), min(sample_size, len(y_true_flow)), replace=False)\n",
    "        y_true_sample = y_true_flow[indices]\n",
    "        y_pred_sample = y_pred_flow[indices].flatten()\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Scatter plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(y_true_sample, y_pred_sample, alpha=0.5)\n",
    "        plt.plot([y_true_sample.min(), y_true_sample.max()], \n",
    "                [y_true_sample.min(), y_true_sample.max()], 'r--', lw=2)\n",
    "        plt.xlabel('Flujo Real (veh/h)')\n",
    "        plt.ylabel('Flujo Predicho (veh/h)')\n",
    "        plt.title('Predicciones vs Valores Reales - Flujo')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Residual plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        residuals = y_true_sample - y_pred_sample\n",
    "        plt.scatter(y_pred_sample, residuals, alpha=0.5)\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.xlabel('Flujo Predicho (veh/h)')\n",
    "        plt.ylabel('Residuales')\n",
    "        plt.title('Gráfico de Residuales')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('predictions_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5301777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_utd19_data(spark, data_path=\"./data/processed/\"):\n",
    "    \"\"\"Función helper para cargar datos ya procesados\"\"\"\n",
    "    print(\"📥 Cargando datos UTD19 limpios...\")\n",
    "\n",
    "    utd19_clean = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(f\"{data_path}utd19clean\")\n",
    "    detectors_clean = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(f\"{data_path}detectorsclean\")\n",
    "    links_clean = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(f\"{data_path}linksclean\")\n",
    "\n",
    "    print(f\"✅ Datos cargados: {utd19_clean.count():,} registros\")\n",
    "    \n",
    "    return utd19_clean, detectors_clean, links_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b83413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FUNCIÓN PRINCIPAL DE EJECUCIÓN =====\n",
    "def main_implementation():\n",
    "    \"\"\"Función principal para ejecutar todo el pipeline\"\"\"\n",
    "    print(\"🚀 INICIANDO IMPLEMENTACIÓN UTD19 BIG DATA + DEEP LEARNING\")\n",
    "    print(\"🎯 ENFOQUE: Real-time traffic management system\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Setup Spark\n",
    "    spark = setup_spark_session()\n",
    "\n",
    "    # 2. ETL Pipeline - Verificar si existen datos limpios\n",
    "    try:\n",
    "        utd19_clean, detectors_clean, links_clean = load_clean_utd19_data(spark)\n",
    "        print(\"✅ Usando datos previamente procesados por ETL\")\n",
    "        ml_df = utd19_clean  # Los datos ya están listos para ML\n",
    "    except:\n",
    "        print(\"⚡ Datos limpios no encontrados. Ejecutando ETL completo...\")\n",
    "        \n",
    "        # Ejecutar ETL completo\n",
    "        etl_pipeline = UTD19_ETL_Pipeline(spark)\n",
    "        clean_datasets = etl_pipeline.run_complete_etl()\n",
    "        \n",
    "        # Usar datos procesados\n",
    "        ml_df = clean_datasets['utd19_ml_ready']\n",
    "        detectors_clean = clean_datasets['detectors']\n",
    "        links_clean = clean_datasets['links']\n",
    "    \n",
    "    # 5. Convertir a Pandas para Deep Learning (muestra)\n",
    "    print(\"\\n📥 Convirtiendo muestra para Deep Learning...\")\n",
    "    sample_size = 1000000  # 1M registros para empezar\n",
    "    ml_df_pandas = ml_df.limit(sample_size).toPandas()\n",
    "    print(f\"✅ Muestra convertida: {len(ml_df_pandas):,} registros\")\n",
    "\n",
    "    X_temp_train, X_temp_test, X_det_train, X_det_test, y_flow_train, y_flow_test, y_cong_train, y_cong_test, scaler, detector_mapping, flow_scaler = split_traffic_data(ml_df_pandas)\n",
    "\n",
    "    # DIAGNÓSTICO CRÍTICO - Agrega esto después de split_traffic_data()\n",
    "    print(\"\\n=== DIAGNÓSTICO DE PROBLEMAS ===\")\n",
    "    \n",
    "    # 1. Analizar distribución de clases\n",
    "    print(\"1. DISTRIBUCIÓN DE CLASES:\")\n",
    "    train_counts = np.bincount(y_cong_train)\n",
    "    test_counts = np.bincount(y_cong_test)\n",
    "    print(f\"Train: {dict(enumerate(train_counts))}\")\n",
    "    print(f\"Test: {dict(enumerate(test_counts))}\")\n",
    "    \n",
    "    # Calcular porcentajes\n",
    "    total_train = len(y_cong_train)\n",
    "    for i, count in enumerate(train_counts):\n",
    "        pct = (count / total_train) * 100\n",
    "        print(f\"Clase {i}: {pct:.1f}%\")\n",
    "    \n",
    "    # 2. Verificar rangos de flow\n",
    "    print(f\"\\n2. RANGOS DE FLOW:\")\n",
    "    print(f\"Train flow - Min: {y_flow_train.min():.3f}, Max: {y_flow_train.max():.3f}\")\n",
    "    print(f\"Test flow - Min: {y_flow_test.min():.3f}, Max: {y_flow_test.max():.3f}\")\n",
    "    \n",
    "    # 3. Verificar features normalizadas\n",
    "    print(f\"\\n3. FEATURES NORMALIZADAS:\")\n",
    "    flat_features = X_temp_train.reshape(-1, X_temp_train.shape[2])\n",
    "    import builtins  # Al inicio de la celda\n",
    "    for i in range(builtins.min(5, flat_features.shape[1])):   # Solo primeras 5\n",
    "        print(f\"Feature {i}: mean={flat_features[:, i].mean():.3f}, std={flat_features[:, i].std():.3f}\")\n",
    "\n",
    "    # DIVISIÓN BALANCEADA TEMPORAL - Agrega esto después del diagnóstico\n",
    "    print(\"\\n=== APLICANDO DIVISIÓN BALANCEADA ===\")\n",
    "    \n",
    "    # Usar stratified split para mantener distribución de clases\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Combinar todos los datos primero\n",
    "    X_temp_all = np.concatenate([X_temp_train, X_temp_test])\n",
    "    X_det_all = np.concatenate([X_det_train, X_det_test])\n",
    "    y_flow_all = np.concatenate([y_flow_train, y_flow_test])\n",
    "    y_cong_all = np.concatenate([y_cong_train, y_cong_test])\n",
    "    \n",
    "    # División estratificada 80/20\n",
    "    X_temp_train, X_temp_test, X_det_train, X_det_test, \\\n",
    "    y_flow_train, y_flow_test, y_cong_train, y_cong_test = train_test_split(\n",
    "        X_temp_all, X_det_all, y_flow_all, y_cong_all,\n",
    "        test_size=0.2, \n",
    "        stratify=y_cong_all, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"NUEVA DISTRIBUCIÓN:\")\n",
    "    print(f\"Train: {dict(enumerate(np.bincount(y_cong_train)))}\")\n",
    "    print(f\"Test: {dict(enumerate(np.bincount(y_cong_test)))}\")\n",
    "\n",
    "    # Y después del train_test_split balanceado, agrega:\n",
    "    print(\"Reetiquetando clases 0→0, 2→1 para el modelo:\")\n",
    "    y_cong_train_relabeled = np.where(y_cong_train == 2, 1, y_cong_train)\n",
    "    y_cong_test_relabeled = np.where(y_cong_test == 2, 1, y_cong_test)\n",
    "    \n",
    "    print(f\"Nueva distribución train: {np.bincount(y_cong_train_relabeled)}\")\n",
    "    print(f\"Nueva distribución test: {np.bincount(y_cong_test_relabeled)}\")\n",
    "    \n",
    "    # Usar las versiones relabeled en el entrenamiento\n",
    "    y_cong_train = y_cong_train_relabeled\n",
    "    y_cong_test = y_cong_test_relabeled\n",
    "\n",
    "    # CALCULAR CLASS WEIGHTS PARA BALANCEAR EL MODELO\n",
    "    print(\"\\n=== CALCULANDO CLASS WEIGHTS ===\")\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    \n",
    "    classes = np.unique(y_cong_train)\n",
    "    class_weights = compute_class_weight('balanced', classes=classes, y=y_cong_train)\n",
    "    class_weight_dict = {int(cls): weight for cls, weight in zip(classes, class_weights)}\n",
    "    print(f\"Class weights calculados: {class_weight_dict}\")\n",
    "    \n",
    "    # Verificar que ahora solo tienes 2 clases (0 y 1)\n",
    "    print(f\"Clases únicas en train: {np.unique(y_cong_train)}\")\n",
    "    print(f\"Clases únicas en test: {np.unique(y_cong_test)}\")\n",
    "\n",
    "    \n",
    "    # 7. Crear y compilar modelo\n",
    "    print(\"\\n🧠 Creando modelo Deep Learning...\")\n",
    "    num_detectors=len(detector_mapping)\n",
    "    sequence_length=12\n",
    "    feature_dim= X_temp_train.shape[2]\n",
    "    # Definir los Input layers\n",
    "    temporal_input = tf.keras.Input(shape=(sequence_length, feature_dim), name='temporal_features')\n",
    "    detector_input = tf.keras.Input(shape=(), dtype=tf.int32, name='detector_ids')\n",
    "    \n",
    "    # Instanciar tu clase\n",
    "    lstm_block = SpatioTemporalLSTM(num_detectors, sequence_length, feature_dim)\n",
    "    outs = lstm_block([temporal_input, detector_input])\n",
    "    # Crear salidas con nombres correctos\n",
    "    flow_out = Lambda(lambda x: x, name='flow_prediction')(outs['flow_prediction'])\n",
    "    cong_out = Lambda(lambda x: x, name='congestion_prediction')(outs['congestion_prediction'])\n",
    "\n",
    "\n",
    "    # Crear el modelo funcional\n",
    "    model = Model(\n",
    "        inputs=[temporal_input, detector_input],\n",
    "        outputs=[flow_out, cong_out],\n",
    "        name=\"spatio_temporal_lstm\"\n",
    "    )\n",
    "    print(\"Model output names:\", model.output_names)\n",
    "    # Compilar con múltiples salidas\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "        loss={\n",
    "            'flow_prediction': MeanSquaredError(),\n",
    "            'congestion_prediction': SparseCategoricalCrossentropy()\n",
    "        },\n",
    "        metrics={\n",
    "            'flow_prediction': [MeanAbsoluteError()],\n",
    "            'congestion_prediction': ['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Modelo creado y compilado\")\n",
    "    # print(f\"📊 Datos preparados para entrenamiento: {X_temporal.shape}\")\n",
    "    \n",
    "    # 9. Entrenar modelo\n",
    "    print(\"\\n🚀 INICIANDO ENTRENAMIENTO...\")\n",
    "\n",
    "    chech_point=ModelCheckpoint('./bestModel/', save_best_only=True, monitor='loss')\n",
    "    print(\"y_cong_train shape:\", y_cong_train.shape, \"dtype:\", y_cong_train.dtype)\n",
    "    print(\"Valores únicos en y_cong_train:\", np.unique(y_cong_train))\n",
    "    print(\"y_flow_train shape:\", y_flow_train.shape, \"dtype:\", y_flow_train.dtype)\n",
    "\n",
    "    print(\"=== Chequeo de features normalizadas ===\")\n",
    "    # Aplana todas las secuencias y calcula min/max por columna\n",
    "    \n",
    "    # Coge solo el eje features\n",
    "    flat = X_temp_train.reshape(-1, X_temp_train.shape[2])  # (n_samples*seq_len, feature_dim)\n",
    "    mins = np.min(flat, axis=0)\n",
    "    maxs = np.max(flat, axis=0)\n",
    "    means = np.mean(flat, axis=0)\n",
    "    stds = np.std(flat, axis=0)\n",
    "    \n",
    "    for i in range(X_temp_train.shape[2]):\n",
    "        print(f\"Feature {i}: min={mins[i]:.3f}, max={maxs[i]:.3f}, mean={means[i]:.3f}, std={stds[i]:.3f}\")\n",
    "\n",
    "\n",
    "    # Crear sample weights basado en las clases\n",
    "    sample_weights = np.array([class_weight_dict[label] for label in y_cong_train])\n",
    "    \n",
    "    print(f\"Sample weights creados: shape={sample_weights.shape}\")\n",
    "    print(f\"Valores únicos de weights: {np.unique(sample_weights)}\")\n",
    "\n",
    "    # Justo después de crear sample_weights y antes de model.fit():\n",
    "    print(\"=== CORRIGIENDO FORMAS PARA SAMPLE_WEIGHT ===\")\n",
    "    y_flow_train = y_flow_train.flatten()  # (799961, 1) -> (799961,)\n",
    "    y_flow_test = y_flow_test.flatten()    # Similar para test\n",
    "    print(f\"Nueva forma y_flow_train: {y_flow_train.shape}\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        x=[X_temp_train, X_det_train],\n",
    "        y={\n",
    "            'flow_prediction': y_flow_train,\n",
    "            'congestion_prediction': y_cong_train\n",
    "        },\n",
    "        validation_data=(\n",
    "            [X_temp_test, X_det_test],\n",
    "            {\n",
    "                'flow_prediction': y_flow_test,\n",
    "                'congestion_prediction': y_cong_test\n",
    "            }\n",
    "        ),\n",
    "        sample_weight=sample_weights,\n",
    "        epochs=10,\n",
    "        batch_size=128,\n",
    "        verbose=1,\n",
    "        callbacks=[chech_point]\n",
    "    )\n",
    "    \n",
    "    print(\"✅ ENTRENAMIENTO COMPLETADO\")\n",
    "\n",
    "    print(\"\\n📊 GENERANDO VISUALIZACIONES...\")\n",
    "    viz = TrafficVisualization()\n",
    "    \n",
    "    # 1. Distribución de datos\n",
    "    viz.plot_data_distribution(ml_df_pandas)\n",
    "    \n",
    "    # 2. Historial de entrenamiento\n",
    "    viz.plot_training_history(history)\n",
    "    \n",
    "    # 3. Predicciones del modelo\n",
    "    print(\"🔮 Generando predicciones para evaluación...\")\n",
    "    predictions = model.predict([X_temp_test, X_det_test])\n",
    "    pred_flow_scaled = predictions[0]      # Primera salida = flow_prediction\n",
    "    pred_cong_scaled = predictions[1]      # Segunda salida = congestion_prediction\n",
    "    pred_flow_real = flow_scaler.inverse_transform(pred_flow_scaled)\n",
    "    print(\"Predicciones (flow) escaladas:\", pred_flow_scaled[:5])\n",
    "    print(\"Predicciones (flow) reales:\", pred_flow_real[:5])\n",
    "\n",
    "    # 🔹 Target real (no escalado)\n",
    "    y_flow_test_real = flow_scaler.inverse_transform(y_flow_test)\n",
    "    \n",
    "    # 🔹 Calcular MAPE real\n",
    "    from sklearn.metrics import mean_absolute_percentage_error\n",
    "    mape = mean_absolute_percentage_error(y_flow_test_real, pred_flow_real) * 100\n",
    "    print(f\"MAPE del flujo en test: {mape:.2f}%\")\n",
    "    \n",
    "    # 4. Matriz de confusión\n",
    "    y_pred_congestion = np.argmax(predictions[1], axis=1)  # predictions[1] = congestion\n",
    "    # Calcular precision, recall y F1 usando sklearn\n",
    "    from sklearn.metrics import precision_score, recall_score\n",
    "    \n",
    "    precision = precision_score(y_cong_test, y_pred_congestion, average='weighted')\n",
    "    recall = recall_score(y_cong_test, y_pred_congestion, average='weighted')\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    print(f\"\\nMÉTRICAS FINALES:\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1-Score: {f1:.3f}\")\n",
    "    viz.plot_confusion_matrix(y_cong_test, y_pred_congestion)\n",
    "    \n",
    "    # 5. Predicciones vs valores reales (flujo)\n",
    "    # 🔹 ahora le pasas ya los reales\n",
    "    viz.plot_predictions_vs_actual(y_flow_test_real, pred_flow_real)\n",
    "    \n",
    "    # 10. EVALUACIÓN CON MÉTRICAS DEL CASO DE ESTUDIO\n",
    "    print(\"\\n📊 EVALUANDO CON MÉTRICAS ESPECÍFICAS DEL CASO...\")\n",
    "    \n",
    "    # Inicializar sistema de métricas\n",
    "    metrics_system = TrafficManagementMetrics()\n",
    "    \n",
    "    # Métrica 1: Prediction Accuracy\n",
    "    prediction_metrics = metrics_system.calculate_prediction_accuracy(\n",
    "        model, [X_temp_test, X_det_test], y_flow_test, y_cong_test\n",
    "    )\n",
    "    \n",
    "    # Preparar datos para métricas 2 y 3 (baseline vs optimizado)\n",
    "    # Simular datos baseline (sin optimización) y optimizados\n",
    "    test_data_baseline = ml_df_pandas.sample(n=10000, random_state=42)\n",
    "    \n",
    "    # Simular mejoras del sistema (velocidades 15% mayores en promedio)\n",
    "    test_data_optimized = test_data_baseline.copy()\n",
    "    test_data_optimized['speed'] = test_data_optimized['speed'] * 1.15\n",
    "    \n",
    "    # Métrica 2: Travel Time Reduction\n",
    "    travel_time_metrics = metrics_system.calculate_travel_time_reduction(\n",
    "        test_data_baseline, test_data_optimized\n",
    "    )\n",
    "    \n",
    "    # Métrica 3: Fuel Savings\n",
    "    fuel_metrics = metrics_system.calculate_fuel_savings(\n",
    "        test_data_baseline, test_data_optimized\n",
    "    )\n",
    "    \n",
    "    # Métrica 4: User Satisfaction\n",
    "    satisfaction_metrics = metrics_system.simulate_user_satisfaction(\n",
    "        travel_time_metrics, fuel_metrics\n",
    "    )\n",
    "    \n",
    "    # Reporte final completo\n",
    "    final_report = metrics_system.generate_final_report(\n",
    "        prediction_metrics, travel_time_metrics, \n",
    "        fuel_metrics, satisfaction_metrics\n",
    "    )\n",
    "    \n",
    "    # Guardar modelo y métricas\n",
    "    model.save_weights('utd19_spatiotemporal_model.h5')\n",
    "    print(\"💾 Modelo guardado: utd19_spatiotemporal_model.h5\")\n",
    "    \n",
    "    # Guardar métricas en JSON\n",
    "    import json\n",
    "    with open('case_study_metrics.json', 'w') as f:\n",
    "        json.dump(final_report, f, indent=2)\n",
    "    print(\"📄 Métricas guardadas: case_study_metrics.json\")\n",
    "    \n",
    "    return model, history, scaler, detector_mapping, final_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "764a8922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 INICIANDO IMPLEMENTACIÓN UTD19 BIG DATA + DEEP LEARNING\n",
      "🎯 ENFOQUE: Real-time traffic management system\n",
      "============================================================\n",
      "✅ Spark configurado - Versión: 3.5.0\n",
      "📊 Configuración: 8 executors, 16GB memoria cada uno\n",
      "📥 Cargando datos UTD19 limpios...\n",
      "✅ Datos cargados: 54,811,846 registros\n",
      "✅ Usando datos previamente procesados por ETL\n",
      "\n",
      "📥 Convirtiendo muestra para Deep Learning...\n",
      "✅ Muestra convertida: 1,000,000 registros\n",
      "🚦 PREPARANDO DATOS PARA PREDICCIÓN DE TRÁFICO\n",
      "📊 Total de registros: 1,000,000\n",
      "📅 Período de entrenamiento: días 2008-05-16 a 2016-11-24 (337 días)\n",
      "📅 Período de prueba: días 2016-11-25 a 2016-12-01 (7 días)\n",
      "🚗 Procesando 4 detectores...\n",
      "✅ Secuencias de entrenamiento: 997,487\n",
      "✅ Secuencias de prueba: 2,465\n",
      "\n",
      "📊 RESUMEN DE DATOS:\n",
      "Train - Temporal: (997487, 12, 12), Flow: (997487,)\n",
      "Test  - Temporal: (2465, 12, 12), Flow: (2465,)\n",
      "\n",
      "🚦 Distribución de congestión:\n",
      "Train: {0: 934481, 1: 0, 2: 63006}\n",
      "Test:  {0: 2465}\n",
      "\n",
      "🌊 Estadísticas de Flow:\n",
      "Train - Min: -0.8, Max: 3.3, Mean: -0.0\n",
      "Test  - Min: -0.5, Max: 3.3, Mean: 2.1\n",
      "\n",
      "=== DIAGNÓSTICO DE PROBLEMAS ===\n",
      "1. DISTRIBUCIÓN DE CLASES:\n",
      "Train: {0: 934481, 1: 0, 2: 63006}\n",
      "Test: {0: 2465}\n",
      "Clase 0: 93.7%\n",
      "Clase 1: 0.0%\n",
      "Clase 2: 6.3%\n",
      "\n",
      "2. RANGOS DE FLOW:\n",
      "Train flow - Min: 0.000, Max: 1.000\n",
      "Test flow - Min: 0.075, Max: 1.000\n",
      "\n",
      "3. FEATURES NORMALIZADAS:\n",
      "Feature 0: mean=-0.005, std=0.994\n",
      "Feature 1: mean=-0.001, std=1.001\n",
      "Feature 2: mean=-0.002, std=0.997\n",
      "Feature 3: mean=0.000, std=1.000\n",
      "Feature 4: mean=0.000, std=1.000\n",
      "\n",
      "=== APLICANDO DIVISIÓN BALANCEADA ===\n",
      "NUEVA DISTRIBUCIÓN:\n",
      "Train: {0: 749556, 1: 0, 2: 50405}\n",
      "Test: {0: 187390, 1: 0, 2: 12601}\n",
      "Reetiquetando clases 0→0, 2→1 para el modelo:\n",
      "Nueva distribución train: [749556  50405]\n",
      "Nueva distribución test: [187390  12601]\n",
      "\n",
      "=== CALCULANDO CLASS WEIGHTS ===\n",
      "Class weights calculados: {0: 0.5336232382904013, 1: 7.935333796250372}\n",
      "Clases únicas en train: [0 1]\n",
      "Clases únicas en test: [0 1]\n",
      "\n",
      "🧠 Creando modelo Deep Learning...\n",
      "Model output names: ['flow_prediction', 'congestion_prediction']\n",
      "✅ Modelo creado y compilado\n",
      "\n",
      "🚀 INICIANDO ENTRENAMIENTO...\n",
      "y_cong_train shape: (799961,) dtype: int32\n",
      "Valores únicos en y_cong_train: [0 1]\n",
      "y_flow_train shape: (799961, 1) dtype: float64\n",
      "=== Chequeo de features normalizadas ===\n",
      "Feature 0: min=-0.845, max=3.287, mean=-0.000, std=1.000\n",
      "Feature 1: min=-2.167, max=1.111, mean=0.000, std=1.000\n",
      "Feature 2: min=-0.676, max=10.329, mean=-0.001, std=0.998\n",
      "Feature 3: min=-1.668, max=1.661, mean=0.000, std=1.000\n",
      "Feature 4: min=-0.579, max=1.727, mean=-0.000, std=1.000\n",
      "Feature 5: min=-0.845, max=3.287, mean=-0.000, std=1.000\n",
      "Feature 6: min=-2.340, max=3.803, mean=-0.000, std=1.000\n",
      "Feature 7: min=-0.569, max=2.485, mean=0.000, std=1.000\n",
      "Feature 8: min=-0.855, max=2.667, mean=-0.000, std=1.000\n",
      "Feature 9: min=-0.830, max=1.205, mean=0.001, std=1.000\n",
      "Feature 10: min=-0.755, max=2.297, mean=-0.000, std=1.000\n",
      "Feature 11: min=-1.456, max=0.687, mean=-0.000, std=1.000\n",
      "Sample weights creados: shape=(799961,)\n",
      "Valores únicos de weights: [0.53362324 7.9353338 ]\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['spatio_temporal_lstm/congestion_prediction/kernel:0', 'spatio_temporal_lstm/congestion_prediction/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['spatio_temporal_lstm/congestion_prediction/kernel:0', 'spatio_temporal_lstm/congestion_prediction/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1131, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1225, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/metrics/confusion_metrics.py\", line 470, in update_state  **\n        return metrics_utils.update_confusion_matrix_variables(\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py\", line 672, in update_confusion_matrix_variables\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n\n    ValueError: Shapes (None, 2) and (None, 1) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     model, history, scaler, detector_mapping, metrics_report \u001b[38;5;241m=\u001b[39m \u001b[43mmain_implementation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🎉 IMPLEMENTACIÓN UTD19 COMPLETADA!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 195\u001b[0m, in \u001b[0;36mmain_implementation\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample weights creados: shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_weights\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValores únicos de weights: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39munique(sample_weights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 195\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mX_temp_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_det_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflow_prediction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_flow_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcongestion_prediction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_cong_train\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_temp_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_det_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflow_prediction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_flow_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcongestion_prediction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_cong_test\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mchech_point\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ ENTRENAMIENTO COMPLETADO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📊 GENERANDO VISUALIZACIONES...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filea883x22g.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1131, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1225, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/metrics/confusion_metrics.py\", line 470, in update_state  **\n        return metrics_utils.update_confusion_matrix_variables(\n    File \"/opt/conda/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py\", line 672, in update_confusion_matrix_variables\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n\n    ValueError: Shapes (None, 2) and (None, 1) are incompatible\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, history, scaler, detector_mapping, metrics_report = main_implementation()\n",
    "    print(\"\\n🎉 IMPLEMENTACIÓN UTD19 COMPLETADA!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"📈 RESULTADOS DEL CASO DE ESTUDIO:\")\n",
    "    print(f\"  🎯 Precisión predicción: {metrics_report['summary']['prediction_accuracy']:.1%}\")\n",
    "    print(f\"  ⏱️ Reducción tiempo viaje: {metrics_report['summary']['travel_time_reduction_pct']:.1f}%\") \n",
    "    print(f\"  ⛽ Ahorro combustible: {metrics_report['summary']['fuel_savings_pct']:.1f}%\")\n",
    "    print(f\"  😊 Satisfacción usuario: {metrics_report['summary']['user_satisfaction']:.1f}/100\")\n",
    "    print(f\"  💰 ROI: {metrics_report['economic_impact']['roi_years']:.1f} años\")\n",
    "    print(\"\\n🚦 Sistema listo para optimización de tráfico en tiempo real!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74914fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3816e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
